{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Playground Simplicity \u00b6 Ultra fast and slim kubernetes playground. Latest News \u00b6 !!! Announcing the Playground SIMPLICITY !!! In a nutshell: Bootstrapping directly from the clouds. It will attempt to upgrade already installed tools to the latest available version. curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash No git clone . No daemon.json configuration. It got a menu :-). Run it via playground from anywhere on your system. Bootstrapping has never been easier! Requirements and Support Matrix \u00b6 The Playground is designed to work on these operating systems Ubuntu Bionic and newer Cloud9 with Ubuntu for a locally running cluster. The deployment scripts for managed cloud clusters are supporting the following cluster types: GKE EKS AKS Supported Cluster Variants \u00b6 Originally, the playground was designed to create a kubernetes cluster locally on the host running the playground scripts. This is still the fastest way of getting a cluster up and running. In addition to the local cluster, it is also possible to use most functionality of the playground on the managed clusters of the main cloud providers AWS, GCP & Azure as well. Going into this direction requires you to work on a Linux shell and an authenticated CLI to the chosen cloud provider ( aws , az or gcloud ). Support Matrix \u00b6 Add-On Ubuntu Local Cloud9 Local GKE Cloud EKS Cloud AKS Cloud Internal Registry X X GCR ECR ACR Scanning Scripts X X X X X C1CS Admission & Continuous X X X X X C1CS Runtime Security X (1) X X X X Falco X X X X X Gatekeeper X X X X X Open Policy Agent X X X X X Prometheus & Grafana X X X X X Trivy X X X X X Cilium X X X X X Kubescape X X X X X Harbor X (2) Smarthome X (2) Pipelines X Jenkins X X GitLab X X Local means, the cluster will run on the machine you're working on. Cloud means, that the cluster is a cloud managed cluster using the named service. (1) Depending on the Kernel in use. Currently the kernels 4.15.x and 5.4.x are supported. (2) In development. CLI Commands of the Playground \u00b6 Besides the obvious cli tools like kubectl , docker , etc. the Playground offers you additional commands shown in the table below (and more): Command Function playground The Playground's menu scan-image Scan an image using Smart Check Example: scan-image nginx:latest scan-namespace Scans all images in use within the current namespace Example: kubectl config set-context --current --namespace <NAMESPACE> scan-namespace collect-logs-sc Collects logs from Smart Check collect-logs-cs Collects logs from Container Security stern Tail logs from multiple pods simultaneously Example: stern -n trendmicro-system . -t -s2m syft See github.com/anchore/syft grype See github.com/anchore/grype k9s See k9scli.io Playgrounds Menu Structure \u00b6 The structure of the menu: playground \u251c\u2500\u2500 Manage Tools and CSPs... \u2502 \u251c\u2500\u2500 Update Tools & Playground \u2502 \u251c\u2500\u2500 Install/Update CLI... \u2502 \u2502 \u251c\u2500\u2500 AWS CLI \u2502 \u2502 \u251c\u2500\u2500 Azure CLI \u2502 \u2502 \u2514\u2500\u2500 GCP CLI \u2502 \u2514\u2500\u2500 Authenticate to CSP... \u2502 \u251c\u2500\u2500 Authenticate to AWS \u2502 \u251c\u2500\u2500 Authenticate to Azure \u2502 \u2514\u2500\u2500 Authenticate to GCP \u251c\u2500\u2500 Manage Clusters... \u2502 \u251c\u2500\u2500 Create a Cluster... \u2502 \u2502 \u251c\u2500\u2500 Local Cluster \u2502 \u2502 \u251c\u2500\u2500 Elastic Kubernetes Cluster \u2502 \u2502 \u251c\u2500\u2500 Azure Kubernetes Cluster \u2502 \u2502 \u2514\u2500\u2500 Google Kubernetes Engine \u2502 \u251c\u2500\u2500 Select Cluster Context... \u2502 \u2502 \u2514\u2500\u2500 (Select a Cluster Context) \u2502 \u2514\u2500\u2500 (Danger Zone) Tear Down Cluster... \u2502 \u251c\u2500\u2500 Local Cluster \u2502 \u251c\u2500\u2500 Elastic Kubernetes Cluster \u2502 \u251c\u2500\u2500 Azure Kubernetes Cluster \u2502 \u2514\u2500\u2500 Google Kubernetes Engine \u251c\u2500\u2500 Manage Services... \u2502 \u251c\u2500\u2500 Deploy Services... \u2502 \u2502 \u2514\u2500\u2500 (Services List) \u2502 \u251c\u2500\u2500 (Danger Zone) Delete Services... \u2502 \u2502 \u2514\u2500\u2500 (Services List) \u2502 \u251c\u2500\u2500 Display Namespaces, LoadBalancers, Deployments & DaemonSets \u2502 \u2514\u2500\u2500 Display Services, Addresses and Credentials \u2514\u2500\u2500 Manage Configuration... \u251c\u2500\u2500 Display Disk Space \u251c\u2500\u2500 Edit Configuration \u2514\u2500\u2500 Edit daemon.json Good to Know \u00b6 If you're curious check out the templates -directory which holds the configuration of all components. Modify at your own risk ;-).","title":"Home"},{"location":"#playground-simplicity","text":"Ultra fast and slim kubernetes playground.","title":"Playground Simplicity"},{"location":"#latest-news","text":"!!! Announcing the Playground SIMPLICITY !!! In a nutshell: Bootstrapping directly from the clouds. It will attempt to upgrade already installed tools to the latest available version. curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash No git clone . No daemon.json configuration. It got a menu :-). Run it via playground from anywhere on your system. Bootstrapping has never been easier!","title":"Latest News"},{"location":"#requirements-and-support-matrix","text":"The Playground is designed to work on these operating systems Ubuntu Bionic and newer Cloud9 with Ubuntu for a locally running cluster. The deployment scripts for managed cloud clusters are supporting the following cluster types: GKE EKS AKS","title":"Requirements and Support Matrix"},{"location":"#supported-cluster-variants","text":"Originally, the playground was designed to create a kubernetes cluster locally on the host running the playground scripts. This is still the fastest way of getting a cluster up and running. In addition to the local cluster, it is also possible to use most functionality of the playground on the managed clusters of the main cloud providers AWS, GCP & Azure as well. Going into this direction requires you to work on a Linux shell and an authenticated CLI to the chosen cloud provider ( aws , az or gcloud ).","title":"Supported Cluster Variants"},{"location":"#support-matrix","text":"Add-On Ubuntu Local Cloud9 Local GKE Cloud EKS Cloud AKS Cloud Internal Registry X X GCR ECR ACR Scanning Scripts X X X X X C1CS Admission & Continuous X X X X X C1CS Runtime Security X (1) X X X X Falco X X X X X Gatekeeper X X X X X Open Policy Agent X X X X X Prometheus & Grafana X X X X X Trivy X X X X X Cilium X X X X X Kubescape X X X X X Harbor X (2) Smarthome X (2) Pipelines X Jenkins X X GitLab X X Local means, the cluster will run on the machine you're working on. Cloud means, that the cluster is a cloud managed cluster using the named service. (1) Depending on the Kernel in use. Currently the kernels 4.15.x and 5.4.x are supported. (2) In development.","title":"Support Matrix"},{"location":"#cli-commands-of-the-playground","text":"Besides the obvious cli tools like kubectl , docker , etc. the Playground offers you additional commands shown in the table below (and more): Command Function playground The Playground's menu scan-image Scan an image using Smart Check Example: scan-image nginx:latest scan-namespace Scans all images in use within the current namespace Example: kubectl config set-context --current --namespace <NAMESPACE> scan-namespace collect-logs-sc Collects logs from Smart Check collect-logs-cs Collects logs from Container Security stern Tail logs from multiple pods simultaneously Example: stern -n trendmicro-system . -t -s2m syft See github.com/anchore/syft grype See github.com/anchore/grype k9s See k9scli.io","title":"CLI Commands of the Playground"},{"location":"#playgrounds-menu-structure","text":"The structure of the menu: playground \u251c\u2500\u2500 Manage Tools and CSPs... \u2502 \u251c\u2500\u2500 Update Tools & Playground \u2502 \u251c\u2500\u2500 Install/Update CLI... \u2502 \u2502 \u251c\u2500\u2500 AWS CLI \u2502 \u2502 \u251c\u2500\u2500 Azure CLI \u2502 \u2502 \u2514\u2500\u2500 GCP CLI \u2502 \u2514\u2500\u2500 Authenticate to CSP... \u2502 \u251c\u2500\u2500 Authenticate to AWS \u2502 \u251c\u2500\u2500 Authenticate to Azure \u2502 \u2514\u2500\u2500 Authenticate to GCP \u251c\u2500\u2500 Manage Clusters... \u2502 \u251c\u2500\u2500 Create a Cluster... \u2502 \u2502 \u251c\u2500\u2500 Local Cluster \u2502 \u2502 \u251c\u2500\u2500 Elastic Kubernetes Cluster \u2502 \u2502 \u251c\u2500\u2500 Azure Kubernetes Cluster \u2502 \u2502 \u2514\u2500\u2500 Google Kubernetes Engine \u2502 \u251c\u2500\u2500 Select Cluster Context... \u2502 \u2502 \u2514\u2500\u2500 (Select a Cluster Context) \u2502 \u2514\u2500\u2500 (Danger Zone) Tear Down Cluster... \u2502 \u251c\u2500\u2500 Local Cluster \u2502 \u251c\u2500\u2500 Elastic Kubernetes Cluster \u2502 \u251c\u2500\u2500 Azure Kubernetes Cluster \u2502 \u2514\u2500\u2500 Google Kubernetes Engine \u251c\u2500\u2500 Manage Services... \u2502 \u251c\u2500\u2500 Deploy Services... \u2502 \u2502 \u2514\u2500\u2500 (Services List) \u2502 \u251c\u2500\u2500 (Danger Zone) Delete Services... \u2502 \u2502 \u2514\u2500\u2500 (Services List) \u2502 \u251c\u2500\u2500 Display Namespaces, LoadBalancers, Deployments & DaemonSets \u2502 \u2514\u2500\u2500 Display Services, Addresses and Credentials \u2514\u2500\u2500 Manage Configuration... \u251c\u2500\u2500 Display Disk Space \u251c\u2500\u2500 Edit Configuration \u2514\u2500\u2500 Edit daemon.json","title":"Playgrounds Menu Structure"},{"location":"#good-to-know","text":"If you're curious check out the templates -directory which holds the configuration of all components. Modify at your own risk ;-).","title":"Good to Know"},{"location":"clusters/","text":"Clusters \u00b6 From within the main menu choose Create Cluster... and select your desired type. Depending on where you have deployed the playground you potentially need to ensure an authenticated cloud CLI. Prerequisites GKE EKS AKS Ubuntu gcloud aws w/ Access Keys az Cloud9 gcloud aws w/ Instance Role (1) az (1) The instance role is automatically created and assigned to the Cloud9 instance during bootstrapping. Then choose your cluster variant to create. If you want to tear down your cluster choose Tear Down Cluster from within the menu. This will destroy the last cluster you created. Note: Cluster versions are defined by the current defaults of the hyper scaler. The built-in cluster is currently version fixed to kubernetes 1.24.7.","title":"Clusters"},{"location":"clusters/#clusters","text":"From within the main menu choose Create Cluster... and select your desired type. Depending on where you have deployed the playground you potentially need to ensure an authenticated cloud CLI. Prerequisites GKE EKS AKS Ubuntu gcloud aws w/ Access Keys az Cloud9 gcloud aws w/ Instance Role (1) az (1) The instance role is automatically created and assigned to the Cloud9 instance during bootstrapping. Then choose your cluster variant to create. If you want to tear down your cluster choose Tear Down Cluster from within the menu. This will destroy the last cluster you created. Note: Cluster versions are defined by the current defaults of the hyper scaler. The built-in cluster is currently version fixed to kubernetes 1.24.7.","title":"Clusters"},{"location":"deployments/","text":"Deployments \u00b6 The playground provides a couple of scripts which deploy pre-configured versions of several products. This includes currently: Container Security Smart Check Prometheus & Grafana Starboard Falco Runtime Security Harbor Open Policy Agent Gatekeeper Deploy the products via Deploy... in the menu. In addition to the above the playground now supports AWS CodePipelines. The pipeline builds a container image based on a sample repo, scans it with Smart Check and deploys it with integrated Cloud One Application Security to the EKS cluster. The pipeline requires an EKS with a deployed Smart Check. If everything has been set up, running the script deploy-pipeline-aws.sh should do the trick :-). When you're done with the pipeline run the generated script pipeline-aws-down.sh to tear it down.","title":"Deployments"},{"location":"deployments/#deployments","text":"The playground provides a couple of scripts which deploy pre-configured versions of several products. This includes currently: Container Security Smart Check Prometheus & Grafana Starboard Falco Runtime Security Harbor Open Policy Agent Gatekeeper Deploy the products via Deploy... in the menu. In addition to the above the playground now supports AWS CodePipelines. The pipeline builds a container image based on a sample repo, scans it with Smart Check and deploys it with integrated Cloud One Application Security to the EKS cluster. The pipeline requires an EKS with a deployed Smart Check. If everything has been set up, running the script deploy-pipeline-aws.sh should do the trick :-). When you're done with the pipeline run the generated script pipeline-aws-down.sh to tear it down.","title":"Deployments"},{"location":"add-ons/cilium/","text":"Add-On: Cilium \u00b6 Install the Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } Install Cilium \u00b6 cilium install Validate the Installation cilium status --wait Enable Hubble in Cilium \u00b6 cilium hubble enable Validate cilium status /\u00af\u00af \\ /\u00af\u00af \\_ _/\u00af\u00af \\ Cilium: OK \\_ _/\u00af\u00af \\_ _/ Operator: OK /\u00af\u00af \\_ _/\u00af\u00af \\ Hubble: OK \\_ _/\u00af\u00af \\_ _/ ClusterMesh: disabled \\_ _/ DaemonSet cilium Desired: 1 , Ready: 1 /1, Available: 1 /1 Deployment cilium-operator Desired: 1 , Ready: 1 /1, Available: 1 /1 Deployment hubble-relay Desired: 1 , Ready: 1 /1, Available: 1 /1 Containers: hubble-relay Running: 1 cilium Running: 1 cilium-operator Running: 1 Cluster Pods: 3 /39 managed by Cilium Image versions cilium quay.io/cilium/cilium:v1.11.3@sha256:cb6aac121e348abd61a692c435a90a6e2ad3f25baa9915346be7b333de8a767f: 1 cilium-operator quay.io/cilium/operator-generic:v1.11.3@sha256:5b81db7a32cb7e2d00bb3cf332277ec2b3be239d9e94a8d979915f4e6648c787: 1 hubble-relay quay.io/cilium/hubble-relay:v1.11.3@sha256:7256ec111259a79b4f0e0f80ba4256ea23bd472e1fc3f0865975c2ed113ccb97: 1 Install the Hubble Client export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Enable the Hubble UI cilium hubble enable --ui Open the Hubble UI cilium hubble ui","title":"Cilium"},{"location":"add-ons/cilium/#add-on-cilium","text":"","title":"Add-On: Cilium"},{"location":"add-ons/cilium/#install-the-cilium-cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"Install the Cilium CLI"},{"location":"add-ons/cilium/#install-cilium","text":"cilium install Validate the Installation cilium status --wait","title":"Install Cilium"},{"location":"add-ons/cilium/#enable-hubble-in-cilium","text":"cilium hubble enable Validate cilium status /\u00af\u00af \\ /\u00af\u00af \\_ _/\u00af\u00af \\ Cilium: OK \\_ _/\u00af\u00af \\_ _/ Operator: OK /\u00af\u00af \\_ _/\u00af\u00af \\ Hubble: OK \\_ _/\u00af\u00af \\_ _/ ClusterMesh: disabled \\_ _/ DaemonSet cilium Desired: 1 , Ready: 1 /1, Available: 1 /1 Deployment cilium-operator Desired: 1 , Ready: 1 /1, Available: 1 /1 Deployment hubble-relay Desired: 1 , Ready: 1 /1, Available: 1 /1 Containers: hubble-relay Running: 1 cilium Running: 1 cilium-operator Running: 1 Cluster Pods: 3 /39 managed by Cilium Image versions cilium quay.io/cilium/cilium:v1.11.3@sha256:cb6aac121e348abd61a692c435a90a6e2ad3f25baa9915346be7b333de8a767f: 1 cilium-operator quay.io/cilium/operator-generic:v1.11.3@sha256:5b81db7a32cb7e2d00bb3cf332277ec2b3be239d9e94a8d979915f4e6648c787: 1 hubble-relay quay.io/cilium/hubble-relay:v1.11.3@sha256:7256ec111259a79b4f0e0f80ba4256ea23bd472e1fc3f0865975c2ed113ccb97: 1 Install the Hubble Client export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Enable the Hubble UI cilium hubble enable --ui Open the Hubble UI cilium hubble ui","title":"Enable Hubble in Cilium"},{"location":"add-ons/container-security/","text":"Add-On: Container Security \u00b6 Deploy \u00b6 Note: The script deploy-container-security.sh uses the Cloud One API Key, which is managed in Cloud One, not Workload Security anymore. If you're not using the new Cloud One API Key and are logging in to Cloud One with an Account Name, e-Mail and Password copy the legacy version of this script found in the legacy -directory over the deploy-container-security.sh script in the root directory. To deploy Container Security run: deploy-container-security.sh deploy-smartcheck.sh Access Container Security \u00b6 Head over to your Cloud One Account and select the Container Security tile. The deployment script automatically creates a policy for your cluster if it doesn't exist already. Some controls in the deploy section are either set to log or block, the continuous section is set to log only. Access Smart Check \u00b6 Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to Smart Check. Linux Example: Smart check UI on: https://192.168.1.121:8443 w/ admin/trendmicro Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browwer. To share Smart Check over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the proxy_listen_port configured in you config.yaml (default: 8443) and choose Source Anywhere Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the proxy_listen_port . If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 8443 (or your configured port), Source 0.0.0.0/0 and Allow. You should now be able to connect to Smart Check on the public ip of your Cloud9 with your configured port. Using a Proxy with Kind and Container Security \u00b6 Overrides \u00b6 Add the following section to your overrides of Container Security: Assumes, that the IP of the docker bridge is 172.17.0.1 if using kind. Otherwise simply put the IP/hostname of the proxy instead. proxy : httpProxy : 172.17.0.1:8081 httpsProxy : 172.17.0.1:8081 noProxy : - localhost - 127.0.0.1 - .cluster.local Kind \u00b6 Set the following environment variables before creating the cluster export HTTP_PROXY = 172 .17.0.1:3128 export HTTPS_PROXY = 172 .17.0.1:3128 export NO_PROXY = localhost,127.0.0.1","title":"Container Security"},{"location":"add-ons/container-security/#add-on-container-security","text":"","title":"Add-On: Container Security"},{"location":"add-ons/container-security/#deploy","text":"Note: The script deploy-container-security.sh uses the Cloud One API Key, which is managed in Cloud One, not Workload Security anymore. If you're not using the new Cloud One API Key and are logging in to Cloud One with an Account Name, e-Mail and Password copy the legacy version of this script found in the legacy -directory over the deploy-container-security.sh script in the root directory. To deploy Container Security run: deploy-container-security.sh deploy-smartcheck.sh","title":"Deploy"},{"location":"add-ons/container-security/#access-container-security","text":"Head over to your Cloud One Account and select the Container Security tile. The deployment script automatically creates a policy for your cluster if it doesn't exist already. Some controls in the deploy section are either set to log or block, the continuous section is set to log only.","title":"Access Container Security"},{"location":"add-ons/container-security/#access-smart-check","text":"Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to Smart Check. Linux Example: Smart check UI on: https://192.168.1.121:8443 w/ admin/trendmicro Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browwer. To share Smart Check over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the proxy_listen_port configured in you config.yaml (default: 8443) and choose Source Anywhere Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the proxy_listen_port . If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 8443 (or your configured port), Source 0.0.0.0/0 and Allow. You should now be able to connect to Smart Check on the public ip of your Cloud9 with your configured port.","title":"Access Smart Check"},{"location":"add-ons/container-security/#using-a-proxy-with-kind-and-container-security","text":"","title":"Using a Proxy with Kind and Container Security"},{"location":"add-ons/container-security/#overrides","text":"Add the following section to your overrides of Container Security: Assumes, that the IP of the docker bridge is 172.17.0.1 if using kind. Otherwise simply put the IP/hostname of the proxy instead. proxy : httpProxy : 172.17.0.1:8081 httpsProxy : 172.17.0.1:8081 noProxy : - localhost - 127.0.0.1 - .cluster.local","title":"Overrides"},{"location":"add-ons/container-security/#kind","text":"Set the following environment variables before creating the cluster export HTTP_PROXY = 172 .17.0.1:3128 export HTTPS_PROXY = 172 .17.0.1:3128 export NO_PROXY = localhost,127.0.0.1","title":"Kind"},{"location":"add-ons/falco/","text":"Add-On: Falco \u00b6 Deploy \u00b6 The deployment of Falco runtime security is very straigt forward with the playground. Simply execute the script deploy-falco.sh , everything else is prepared. deploy-falco.sh To ignore events triggerd by services belonging to the playground environment, you can easily whitelist all the playground components by running the following script. whitelist_playground_ns.sh Rerun the script whenever you're deploying additional playground components. Falco is integrated with Prometheus and Grafana as well. A Dashboard is available for import with the ID 11914. Access \u00b6 Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to smartcheck. Linux By default, the Falco UI is on port 8082. Example: Falco UI on: http://192.168.1.121:8082/ui/#/ Cloud9 See: Access Smart Check (Cloud9) , but use the proxy_listen_port s configured in your config.yaml (default: 8082) and choose Source Anywhere. Don't forget to check your inbound rules to allow the port. Alternatively, you can get the configured port for the service with cat services . Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s). Examle Falco UI: <http://YOUR-CLOUD9-PUBLIC-IP:8082/ui/#/> Try it \u00b6 To test the k8s auditing try to create a configmap: cat <<EOF | kubectl apply -f - apiVersion: v1 data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true access.properties: | aws_access_key_id = AKIAXXXXXXXXXXXXXXXX aws_secret_access_key = 1CHPXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX kind: ConfigMap metadata: name: awscfg EOF If you want to test out own falco rules, create a file called falco/additional_rules.yaml write your rules. It will be included when running deploy-falco.sh . Example: - macro : container condition : container.id != host - macro : spawned_process condition : evt.type = execve and evt.dir=< - rule : (AR) Run shell in container desc : a shell was spawned by a non-shell program in a container. Container entrypoints are excluded. condition : container and proc.name = bash and spawned_process and proc.pname exists and not proc.pname in (bash, docker) output : \"Shell spawned in a container other than entrypoint (user=%user.name container_id=%container.id container_name=%container.name shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)\" priority : WARNING Generate some events \u00b6 docker run -it --rm falcosecurity/event-generator run syscall --loop Fun with privileged mode \u00b6 function shell () { kubectl run shell --restart = Never -it --image mawinkler/kod:latest \\ --rm --attach \\ --overrides \\ ' { \"spec\":{ \"hostPID\": true, \"containers\":[{ \"name\":\"kod\", \"image\": \"mawinkler/kod:latest\", \"imagePullPolicy\": \"Always\", \"stdin\": true, \"tty\": true, \"command\":[\"/bin/bash\"], \"nodeSelector\":{ \"dedicated\":\"master\" }, \"securityContext\":{ \"privileged\":true } }] } } ' } You can paste this into a new file shell.sh and source the file. . ./shell.sh Then you can type the following to demonstrate a privilege escalation in Kubernetes. shell If you don't see a command prompt, try pressing enter. root@shell:/# godmode root@playground-control-plane:/# You're now on the control plane of the cluster and should be kubernetes-admin. If you're wondering what you can do now... mkdir -p /root/.kube cp /etc/kubernetes/admin.conf /root/.kube/config kubectl auth can-i create deployments -n kube-system kubectl create deployment echo --image = inanimate/echo-server kubectl get pods","title":"Falco"},{"location":"add-ons/falco/#add-on-falco","text":"","title":"Add-On: Falco"},{"location":"add-ons/falco/#deploy","text":"The deployment of Falco runtime security is very straigt forward with the playground. Simply execute the script deploy-falco.sh , everything else is prepared. deploy-falco.sh To ignore events triggerd by services belonging to the playground environment, you can easily whitelist all the playground components by running the following script. whitelist_playground_ns.sh Rerun the script whenever you're deploying additional playground components. Falco is integrated with Prometheus and Grafana as well. A Dashboard is available for import with the ID 11914.","title":"Deploy"},{"location":"add-ons/falco/#access","text":"Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to smartcheck. Linux By default, the Falco UI is on port 8082. Example: Falco UI on: http://192.168.1.121:8082/ui/#/ Cloud9 See: Access Smart Check (Cloud9) , but use the proxy_listen_port s configured in your config.yaml (default: 8082) and choose Source Anywhere. Don't forget to check your inbound rules to allow the port. Alternatively, you can get the configured port for the service with cat services . Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s). Examle Falco UI: <http://YOUR-CLOUD9-PUBLIC-IP:8082/ui/#/>","title":"Access"},{"location":"add-ons/falco/#try-it","text":"To test the k8s auditing try to create a configmap: cat <<EOF | kubectl apply -f - apiVersion: v1 data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true access.properties: | aws_access_key_id = AKIAXXXXXXXXXXXXXXXX aws_secret_access_key = 1CHPXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX kind: ConfigMap metadata: name: awscfg EOF If you want to test out own falco rules, create a file called falco/additional_rules.yaml write your rules. It will be included when running deploy-falco.sh . Example: - macro : container condition : container.id != host - macro : spawned_process condition : evt.type = execve and evt.dir=< - rule : (AR) Run shell in container desc : a shell was spawned by a non-shell program in a container. Container entrypoints are excluded. condition : container and proc.name = bash and spawned_process and proc.pname exists and not proc.pname in (bash, docker) output : \"Shell spawned in a container other than entrypoint (user=%user.name container_id=%container.id container_name=%container.name shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)\" priority : WARNING","title":"Try it"},{"location":"add-ons/falco/#generate-some-events","text":"docker run -it --rm falcosecurity/event-generator run syscall --loop","title":"Generate some events"},{"location":"add-ons/falco/#fun-with-privileged-mode","text":"function shell () { kubectl run shell --restart = Never -it --image mawinkler/kod:latest \\ --rm --attach \\ --overrides \\ ' { \"spec\":{ \"hostPID\": true, \"containers\":[{ \"name\":\"kod\", \"image\": \"mawinkler/kod:latest\", \"imagePullPolicy\": \"Always\", \"stdin\": true, \"tty\": true, \"command\":[\"/bin/bash\"], \"nodeSelector\":{ \"dedicated\":\"master\" }, \"securityContext\":{ \"privileged\":true } }] } } ' } You can paste this into a new file shell.sh and source the file. . ./shell.sh Then you can type the following to demonstrate a privilege escalation in Kubernetes. shell If you don't see a command prompt, try pressing enter. root@shell:/# godmode root@playground-control-plane:/# You're now on the control plane of the cluster and should be kubernetes-admin. If you're wondering what you can do now... mkdir -p /root/.kube cp /etc/kubernetes/admin.conf /root/.kube/config kubectl auth can-i create deployments -n kube-system kubectl create deployment echo --image = inanimate/echo-server kubectl get pods","title":"Fun with privileged mode"},{"location":"add-ons/gatekeeper/","text":"Add-On: Gatekeeper \u00b6 Deploy \u00b6 Gatekeeper is a customizable admission webhook for Kubernetes that dynamically enforces policies executed by the OPA. Gatekeeper uses CustomResourceDefinitions internally and allows us to define ConstraintTemplates and Constraints to enforce policies on Kubernetes resources such as Pods, Deployments, Jobs. OPA/Gatekeeper uses its own declarative language called Rego, a query language. You define rules in Rego which, if invalid or returned a false expression, will trigger a constraint violation and blocks the ongoing process of creating/updating/deleting the resource. ConstraintTemplate A ConstraintTemplate consists of both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint. Constraint Constraint is an object that says on which resources are the policies applicable, and also what parameters are to be queried and checked to see if they are available in the resource manifest the user is trying to apply in your Kubernetes cluster. Simply put, it is a declaration that its author wants the system to meet a given set of requirements. To deploy run: deploy-gatekeeper.sh Usage \u00b6 Example Policy: Namespace Label Mandates \u00b6 There is an example within the gatekeeper directory which you can apply by doing kubectl apply -f gatekeeper/constrainttemplate.yaml kubectl apply -f gatekeeper/constraints.yaml From now on, any new namespace being created requires labels set for stage , status and zone . To test it, run kubectl create namespace nginx --dry-run = true -o yaml | kubectl apply -f - Error from server ([label-check] DENIED. Reason: Our org policy mandates the following labels: You must provide these labels: {\"stage\", \"status\", \"zone\"}): error when creating \"STDIN\": admission webhook \"validation.gatekeeper.sh\" denied the request: [label-check] DENIED. Reason: Our org policy mandates the following labels: You must provide these labels: {\"stage\", \"status\", \"zone\"} A valid namespace definition could look like the following: cat <<EOF | kubectl apply -f - -o yaml apiVersion: v1 kind: Namespace metadata: name: nginx labels: zone: eu-central-1 stage: dev status: ready EOF A subsequent nginx deployment can be done via: kubectl create deployment --image = nginx --namespace nginx nginx","title":"Gatekeeper"},{"location":"add-ons/gatekeeper/#add-on-gatekeeper","text":"","title":"Add-On: Gatekeeper"},{"location":"add-ons/gatekeeper/#deploy","text":"Gatekeeper is a customizable admission webhook for Kubernetes that dynamically enforces policies executed by the OPA. Gatekeeper uses CustomResourceDefinitions internally and allows us to define ConstraintTemplates and Constraints to enforce policies on Kubernetes resources such as Pods, Deployments, Jobs. OPA/Gatekeeper uses its own declarative language called Rego, a query language. You define rules in Rego which, if invalid or returned a false expression, will trigger a constraint violation and blocks the ongoing process of creating/updating/deleting the resource. ConstraintTemplate A ConstraintTemplate consists of both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint. Constraint Constraint is an object that says on which resources are the policies applicable, and also what parameters are to be queried and checked to see if they are available in the resource manifest the user is trying to apply in your Kubernetes cluster. Simply put, it is a declaration that its author wants the system to meet a given set of requirements. To deploy run: deploy-gatekeeper.sh","title":"Deploy"},{"location":"add-ons/gatekeeper/#usage","text":"","title":"Usage"},{"location":"add-ons/gatekeeper/#example-policy-namespace-label-mandates","text":"There is an example within the gatekeeper directory which you can apply by doing kubectl apply -f gatekeeper/constrainttemplate.yaml kubectl apply -f gatekeeper/constraints.yaml From now on, any new namespace being created requires labels set for stage , status and zone . To test it, run kubectl create namespace nginx --dry-run = true -o yaml | kubectl apply -f - Error from server ([label-check] DENIED. Reason: Our org policy mandates the following labels: You must provide these labels: {\"stage\", \"status\", \"zone\"}): error when creating \"STDIN\": admission webhook \"validation.gatekeeper.sh\" denied the request: [label-check] DENIED. Reason: Our org policy mandates the following labels: You must provide these labels: {\"stage\", \"status\", \"zone\"} A valid namespace definition could look like the following: cat <<EOF | kubectl apply -f - -o yaml apiVersion: v1 kind: Namespace metadata: name: nginx labels: zone: eu-central-1 stage: dev status: ready EOF A subsequent nginx deployment can be done via: kubectl create deployment --image = nginx --namespace nginx nginx","title":"Example Policy: Namespace Label Mandates"},{"location":"add-ons/gitlab/","text":"Add-On: GitLab \u00b6 Deploy \u00b6 Note: The script deploy-gitlab.sh deploys a GitLab with Docker Pipeline Support directly on the Docker engine. In other words, GitLab does NOT run on Kubernetes but locally on your machine. This is because GitLab needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind. To deploy GitLab run: deploy-gitlab.sh GitLabs configuration is stored on the host within the directories /srv/gitlab and /srv/gitlab-runner . The configuration survives restart and a full delete-deploy operation. If you need to restart from scratch delete the two directories. Access GitLab \u00b6 By default GitLab listens on Port 80. The login credentials for the initial login are reported in the Services item of the Playground Menu. http://localhost Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share GitLab over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the GitLab port ( 80 ) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course) Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 80 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow. You should now be able to connect to GitLab on the public ip of your Cloud9 with your configured port. Configure GitLab \u00b6 Depending on what you're going to do with GitLab some little configuration steps are typically required. Environment Variables CI_COMMIT_BRANCH: main CI_DEFAULT_BRANCH: main CI_REGISTRY_IMAGE: mawinkler/helloworld CI_REGISTRY_PASSWORD: DOCKER_PASSWORD CI_REGISTRY_USER: mawinkler Pipeline: # Build a Docker image with CI/CD and push to the GitLab registry. # Docker-in-Docker documentation: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html # # This template uses one generic job with conditional builds # for the default branch and all other (MR) branches. docker-build : # Use the official docker image. image : docker:latest stage : build services : - docker:dind before_script : - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" #$CI_REGISTRY # Default branch leaves tag empty (= latest tag) # All other branches are tagged with the escaped branch name (commit ref slug) script : - | if [[ \"$CI_COMMIT_BRANCH\" == \"$CI_DEFAULT_BRANCH\" ]]; then tag=\"\" echo \"Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'\" else tag=\":$CI_COMMIT_REF_SLUG\" echo \"Running on branch '$CI_COMMIT_BRANCH': tag = $tag\" fi - docker build --pull -t \"$CI_REGISTRY_IMAGE${tag}\" . - docker push \"$CI_REGISTRY_IMAGE${tag}\" # Run this job in a branch where a Dockerfile exists rules : - if : $CI_COMMIT_BRANCH exists : - Dockerfile tags : - docker Dockerfile: FROM ubuntu COPY helloworld.sh / RUN chmod 700 /helloworld.sh ENTRYPOINT /helloworld.sh Script: #!/bin/bash echo HelloWorld script within Docker image exit 0","title":"GitLab"},{"location":"add-ons/gitlab/#add-on-gitlab","text":"","title":"Add-On: GitLab"},{"location":"add-ons/gitlab/#deploy","text":"Note: The script deploy-gitlab.sh deploys a GitLab with Docker Pipeline Support directly on the Docker engine. In other words, GitLab does NOT run on Kubernetes but locally on your machine. This is because GitLab needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind. To deploy GitLab run: deploy-gitlab.sh GitLabs configuration is stored on the host within the directories /srv/gitlab and /srv/gitlab-runner . The configuration survives restart and a full delete-deploy operation. If you need to restart from scratch delete the two directories.","title":"Deploy"},{"location":"add-ons/gitlab/#access-gitlab","text":"By default GitLab listens on Port 80. The login credentials for the initial login are reported in the Services item of the Playground Menu. http://localhost Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share GitLab over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the GitLab port ( 80 ) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course) Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 80 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow. You should now be able to connect to GitLab on the public ip of your Cloud9 with your configured port.","title":"Access GitLab"},{"location":"add-ons/gitlab/#configure-gitlab","text":"Depending on what you're going to do with GitLab some little configuration steps are typically required. Environment Variables CI_COMMIT_BRANCH: main CI_DEFAULT_BRANCH: main CI_REGISTRY_IMAGE: mawinkler/helloworld CI_REGISTRY_PASSWORD: DOCKER_PASSWORD CI_REGISTRY_USER: mawinkler Pipeline: # Build a Docker image with CI/CD and push to the GitLab registry. # Docker-in-Docker documentation: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html # # This template uses one generic job with conditional builds # for the default branch and all other (MR) branches. docker-build : # Use the official docker image. image : docker:latest stage : build services : - docker:dind before_script : - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" #$CI_REGISTRY # Default branch leaves tag empty (= latest tag) # All other branches are tagged with the escaped branch name (commit ref slug) script : - | if [[ \"$CI_COMMIT_BRANCH\" == \"$CI_DEFAULT_BRANCH\" ]]; then tag=\"\" echo \"Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'\" else tag=\":$CI_COMMIT_REF_SLUG\" echo \"Running on branch '$CI_COMMIT_BRANCH': tag = $tag\" fi - docker build --pull -t \"$CI_REGISTRY_IMAGE${tag}\" . - docker push \"$CI_REGISTRY_IMAGE${tag}\" # Run this job in a branch where a Dockerfile exists rules : - if : $CI_COMMIT_BRANCH exists : - Dockerfile tags : - docker Dockerfile: FROM ubuntu COPY helloworld.sh / RUN chmod 700 /helloworld.sh ENTRYPOINT /helloworld.sh Script: #!/bin/bash echo HelloWorld script within Docker image exit 0","title":"Configure GitLab"},{"location":"add-ons/harbor/","text":"Add-On: Harbor \u00b6 Deploy \u00b6 Note: Harbor deployment is currently only supported for local Ubuntu Playgrounds. To deploy Harbor run: deploy-harbor.sh Access \u00b6 Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to Harbor. Linux Example: Service harbor on: https://192.168.1.121:8085 w/ admin/trendmicro Integrate Harbor to Smart Check \u00b6 Add the Harbor registry as a Generic Registry. On the local Playground the IP address should be 172.250.255.5, port is 443. Before doing the integration you need to add a user to harbor with admin privileges ( Administration --> User ) and add this user to the available projects as a member.","title":"Harbor"},{"location":"add-ons/harbor/#add-on-harbor","text":"","title":"Add-On: Harbor"},{"location":"add-ons/harbor/#deploy","text":"Note: Harbor deployment is currently only supported for local Ubuntu Playgrounds. To deploy Harbor run: deploy-harbor.sh","title":"Deploy"},{"location":"add-ons/harbor/#access","text":"Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to Harbor. Linux Example: Service harbor on: https://192.168.1.121:8085 w/ admin/trendmicro","title":"Access"},{"location":"add-ons/harbor/#integrate-harbor-to-smart-check","text":"Add the Harbor registry as a Generic Registry. On the local Playground the IP address should be 172.250.255.5, port is 443. Before doing the integration you need to add a user to harbor with admin privileges ( Administration --> User ) and add this user to the available projects as a member.","title":"Integrate Harbor to Smart Check"},{"location":"add-ons/istio/","text":"Istio \u00b6 Deploy \u00b6 # Add repo helm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update # Install the Istio base chart kubectl create namespace istio-system helm install istio-base istio/base -n istio-system # Install the Istio discovery chart which deploys the istiod service helm install istiod istio/istiod -n istio-system --wait # (Optional) Install an ingress gateway kubectl create namespace istio-ingress kubectl label namespace istio-ingress istio-injection = enabled helm install istio-ingress istio/gateway -n istio-ingress --wait # Verified using Helm helm status istiod -n istio-system","title":"Istio"},{"location":"add-ons/istio/#istio","text":"","title":"Istio"},{"location":"add-ons/istio/#deploy","text":"# Add repo helm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update # Install the Istio base chart kubectl create namespace istio-system helm install istio-base istio/base -n istio-system # Install the Istio discovery chart which deploys the istiod service helm install istiod istio/istiod -n istio-system --wait # (Optional) Install an ingress gateway kubectl create namespace istio-ingress kubectl label namespace istio-ingress istio-injection = enabled helm install istio-ingress istio/gateway -n istio-ingress --wait # Verified using Helm helm status istiod -n istio-system","title":"Deploy"},{"location":"add-ons/jenkins/","text":"Add-On: Jenkins \u00b6 Deploy \u00b6 Note: The script deploy-jenkins.sh deploys a Jenkins with Docker Pipeline Support and BlueOcean directly on the Docker engine. In other words, Jenkins does NOT run on Kubernetes but locally on your machine. This is because Jenkins needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind. To deploy Jenkins run: deploy-jenkins.sh Access Jenkins \u00b6 By default Jenkins listens on Port 8080. The login credentials for the initial login are reported in the Services item of the Playground Menu. The password is for one time use only. After logging in to http://localhost:8080 You will be prompted for it. In the following you need to create the initial admin user. There are no password policies set, so an admin / trendmicro will work :-). Next, deploy the recommended plug-ins and head over to the Jenkins console. Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share Jenkins over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the Jenkins port ( 8080 ) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course) Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 8080 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow. You should now be able to connect to Jenkins on the public ip of your Cloud9 with your configured port. Configure Jenkins \u00b6 Depending on what you're going to do with Jenkins some little configuration steps are typically required. I believe the required steps are best demonstrated by an example. Let's assume we want to scan an image from ACR with Smart Check and want to get a pdf report created if the scan reports vulnerability findings. The report is then filed as an artifact within the pipeline run. The pipeline will require two credentials: Azure Container Registry Smart Check To create them in Jenkins head over to Manage Jenkins --> Manage Credentials . Create two crendentials in the global domain as Username with password . Name the one for ACR as acr-auth and the smart check one as smartcheck-auth . Additionally, we need to tell Jenkins where to find Smart Check. For this we do create an environment variable. So head over to Manage Jenkins --> Configure System . Scroll down to the Global Properties , check Environment variables and [Add] . Name the variable as DSSC_SERVICE with the IP:Port of your Smart Check instance (e.g. <ip of your server>:8443 which is the address of the nginx proxy deployed by the Smart Check deployment script forwarding to your Smart Check running on a kind Kubernetes cluster). Hit [Save] . Then, back on the main dashboard of Jenkins hit + New Item , enter an item name (e.g. ACR Scan ) and click on Pipeline followed by [OK] . For our simple example you can leave all the options unchecked. Into the Pipeline -section paste the following groovy code: pipeline { agent any stages { stage ( 'Test' ) { steps { withCredentials ([ usernamePassword ( credentialsId: 'smartcheck-auth' , usernameVariable: 'SMARTCHECK_AUTH_CREDS_USR' , passwordVariable: 'SMARTCHECK_AUTH_CREDS_PSW' ), usernamePassword ( credentialsId: 'acr-auth' , usernameVariable: 'ACR_AUTH_CREDS_USR' , passwordVariable: 'ACR_AUTH_CREDS_PSW' ) ]) { script { try { sh \"\"\" docker run deepsecurity/smartcheck-scan-action \\ --image-name astrolive.azurecr.io/astrolive:latest \\ --smartcheck-host=$DSSC_SERVICE \\ --smartcheck-user=$SMARTCHECK_AUTH_CREDS_USR \\ --smartcheck-password=$SMARTCHECK_AUTH_CREDS_PSW \\ --insecure-skip-tls-verify=true \\ --image-pull-auth=\\'{\"username\": \"$ACR_AUTH_CREDS_USR\", \"password\": \"$ACR_AUTH_CREDS_PSW\"}\\' \"\"\" } catch ( e ) { script { docker . image ( 'mawinkler/scan-report' ). pull () docker . image ( 'mawinkler/scan-report' ). inside ( \"--entrypoint=''\" ) { sh \"\"\" python /usr/src/app/scan-report.py \\ --config_path \"/usr/src/app\" \\ --name \"astrolive\" \\ --image_tag \"latest\" \\ --out_path \"${WORKSPACE}\" \\ --service \"${DSSC_SERVICE}\" \\ --username \"${SMARTCHECK_AUTH_CREDS_USR}\" \\ --password \"${SMARTCHECK_AUTH_CREDS_PSW}\" \"\"\" archiveArtifacts artifacts: 'report_*.pdf' } error ( 'Issues in image found' ) } } } } } } } } Hit [Save] and start the build with Build Now . If you want see the build progress click on the currently running build in the lower left area of your Jenkins. Within the Console Output you should see the output generated by your first Jenkins pipeline :-).","title":"Jenkins"},{"location":"add-ons/jenkins/#add-on-jenkins","text":"","title":"Add-On: Jenkins"},{"location":"add-ons/jenkins/#deploy","text":"Note: The script deploy-jenkins.sh deploys a Jenkins with Docker Pipeline Support and BlueOcean directly on the Docker engine. In other words, Jenkins does NOT run on Kubernetes but locally on your machine. This is because Jenkins needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind. To deploy Jenkins run: deploy-jenkins.sh","title":"Deploy"},{"location":"add-ons/jenkins/#access-jenkins","text":"By default Jenkins listens on Port 8080. The login credentials for the initial login are reported in the Services item of the Playground Menu. The password is for one time use only. After logging in to http://localhost:8080 You will be prompted for it. In the following you need to create the initial admin user. There are no password policies set, so an admin / trendmicro will work :-). Next, deploy the recommended plug-ins and head over to the Jenkins console. Cloud9 If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share Jenkins over the internet, follow the steps below. Query the public IP of your Cloud9 instance with curl http://169.254.169.254/latest/meta-data/public-ipv4 In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance Select the security group associated to the instance and select Edit inbound rules. Add an inbound rule for the Jenkins port ( 8080 ) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course) Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL. Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on Edit inbound rules and add a rule with a low Rule number, Custom TCP, Port range 8080 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow. You should now be able to connect to Jenkins on the public ip of your Cloud9 with your configured port.","title":"Access Jenkins"},{"location":"add-ons/jenkins/#configure-jenkins","text":"Depending on what you're going to do with Jenkins some little configuration steps are typically required. I believe the required steps are best demonstrated by an example. Let's assume we want to scan an image from ACR with Smart Check and want to get a pdf report created if the scan reports vulnerability findings. The report is then filed as an artifact within the pipeline run. The pipeline will require two credentials: Azure Container Registry Smart Check To create them in Jenkins head over to Manage Jenkins --> Manage Credentials . Create two crendentials in the global domain as Username with password . Name the one for ACR as acr-auth and the smart check one as smartcheck-auth . Additionally, we need to tell Jenkins where to find Smart Check. For this we do create an environment variable. So head over to Manage Jenkins --> Configure System . Scroll down to the Global Properties , check Environment variables and [Add] . Name the variable as DSSC_SERVICE with the IP:Port of your Smart Check instance (e.g. <ip of your server>:8443 which is the address of the nginx proxy deployed by the Smart Check deployment script forwarding to your Smart Check running on a kind Kubernetes cluster). Hit [Save] . Then, back on the main dashboard of Jenkins hit + New Item , enter an item name (e.g. ACR Scan ) and click on Pipeline followed by [OK] . For our simple example you can leave all the options unchecked. Into the Pipeline -section paste the following groovy code: pipeline { agent any stages { stage ( 'Test' ) { steps { withCredentials ([ usernamePassword ( credentialsId: 'smartcheck-auth' , usernameVariable: 'SMARTCHECK_AUTH_CREDS_USR' , passwordVariable: 'SMARTCHECK_AUTH_CREDS_PSW' ), usernamePassword ( credentialsId: 'acr-auth' , usernameVariable: 'ACR_AUTH_CREDS_USR' , passwordVariable: 'ACR_AUTH_CREDS_PSW' ) ]) { script { try { sh \"\"\" docker run deepsecurity/smartcheck-scan-action \\ --image-name astrolive.azurecr.io/astrolive:latest \\ --smartcheck-host=$DSSC_SERVICE \\ --smartcheck-user=$SMARTCHECK_AUTH_CREDS_USR \\ --smartcheck-password=$SMARTCHECK_AUTH_CREDS_PSW \\ --insecure-skip-tls-verify=true \\ --image-pull-auth=\\'{\"username\": \"$ACR_AUTH_CREDS_USR\", \"password\": \"$ACR_AUTH_CREDS_PSW\"}\\' \"\"\" } catch ( e ) { script { docker . image ( 'mawinkler/scan-report' ). pull () docker . image ( 'mawinkler/scan-report' ). inside ( \"--entrypoint=''\" ) { sh \"\"\" python /usr/src/app/scan-report.py \\ --config_path \"/usr/src/app\" \\ --name \"astrolive\" \\ --image_tag \"latest\" \\ --out_path \"${WORKSPACE}\" \\ --service \"${DSSC_SERVICE}\" \\ --username \"${SMARTCHECK_AUTH_CREDS_USR}\" \\ --password \"${SMARTCHECK_AUTH_CREDS_PSW}\" \"\"\" archiveArtifacts artifacts: 'report_*.pdf' } error ( 'Issues in image found' ) } } } } } } } } Hit [Save] and start the build with Build Now . If you want see the build progress click on the currently running build in the lower left area of your Jenkins. Within the Console Output you should see the output generated by your first Jenkins pipeline :-).","title":"Configure Jenkins"},{"location":"add-ons/krew/","text":"Add-On: Krew \u00b6 Usage \u00b6 Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew. Today, over 130 kubectl plugins are available on Krew. Example usage: kubectl krew list kubectl krew install tree The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree. kubectl tree node playground-control-plane -A","title":"Krew"},{"location":"add-ons/krew/#add-on-krew","text":"","title":"Add-On: Krew"},{"location":"add-ons/krew/#usage","text":"Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew. Today, over 130 kubectl plugins are available on Krew. Example usage: kubectl krew list kubectl krew install tree The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree. kubectl tree node playground-control-plane -A","title":"Usage"},{"location":"add-ons/kubescape/","text":"Add-On: Kubescape \u00b6 Install the Kubescape CLI \u00b6 curl -s https://raw.githubusercontent.com/armosec/kubescape/master/install.sh | /bin/bash Run a full Scan \u00b6 export ARMO_ACCOUNT = XXX kubescape scan framework nsa,mitre,armobest \\ --submit --enable-host-scan --format-version v2 --verbose \\ --exclude-namespaces = kube-system --fail-threshold 0 \\ --account ${ ARMO_ACCOUNT } --submit","title":"Kubescape"},{"location":"add-ons/kubescape/#add-on-kubescape","text":"","title":"Add-On: Kubescape"},{"location":"add-ons/kubescape/#install-the-kubescape-cli","text":"curl -s https://raw.githubusercontent.com/armosec/kubescape/master/install.sh | /bin/bash","title":"Install the Kubescape CLI"},{"location":"add-ons/kubescape/#run-a-full-scan","text":"export ARMO_ACCOUNT = XXX kubescape scan framework nsa,mitre,armobest \\ --submit --enable-host-scan --format-version v2 --verbose \\ --exclude-namespaces = kube-system --fail-threshold 0 \\ --account ${ ARMO_ACCOUNT } --submit","title":"Run a full Scan"},{"location":"add-ons/opa/","text":"Add-On: Open Policy Agent \u00b6 Deploy \u00b6 The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. You don\u2019t have to write policies on your own at the beginning of your journey, OPA and Gatekeeper both have excellent community libraries. You can have a look, fork them, and use them in your organization from here, OPA , and Gatekeeper libraries. To deploy the registry run: deploy-opa.sh Usage \u00b6 Example Policy: Registry Whitelisting \u00b6 cat <<EOF >opa/registry-whitelist.rego package kubernetes.admission deny[msg] { input.request.kind.kind == \"Pod\" image := input.request.object.spec.containers[_].image not startswith(image, \"172.18.255.1/\") msg := sprintf(\"Image is not from our trusted cluster registry: %v\", [image]) } EOF kubectl -n opa create configmap registry-whitelist --from-file = opa/registry-whitelist.rego Try to create a deployment kubectl create deployment echo --image = inanimate/echo-server If you now run a kubectl get pods , the echo-server should NOT show up. Access the logs from OPA kubectl -n opa logs -l app = opa -c opa -f There should be something like \"message\" : \"Error creating: admission webhook \\\"validating-webhook.openpolicyagent.org\\\" denied the request: Image is not from our trusted cluster registry: inanimate/echo-server\" ,","title":"Open Policy Agent"},{"location":"add-ons/opa/#add-on-open-policy-agent","text":"","title":"Add-On: Open Policy Agent"},{"location":"add-ons/opa/#deploy","text":"The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. You don\u2019t have to write policies on your own at the beginning of your journey, OPA and Gatekeeper both have excellent community libraries. You can have a look, fork them, and use them in your organization from here, OPA , and Gatekeeper libraries. To deploy the registry run: deploy-opa.sh","title":"Deploy"},{"location":"add-ons/opa/#usage","text":"","title":"Usage"},{"location":"add-ons/opa/#example-policy-registry-whitelisting","text":"cat <<EOF >opa/registry-whitelist.rego package kubernetes.admission deny[msg] { input.request.kind.kind == \"Pod\" image := input.request.object.spec.containers[_].image not startswith(image, \"172.18.255.1/\") msg := sprintf(\"Image is not from our trusted cluster registry: %v\", [image]) } EOF kubectl -n opa create configmap registry-whitelist --from-file = opa/registry-whitelist.rego Try to create a deployment kubectl create deployment echo --image = inanimate/echo-server If you now run a kubectl get pods , the echo-server should NOT show up. Access the logs from OPA kubectl -n opa logs -l app = opa -c opa -f There should be something like \"message\" : \"Error creating: admission webhook \\\"validating-webhook.openpolicyagent.org\\\" denied the request: Image is not from our trusted cluster registry: inanimate/echo-server\" ,","title":"Example Policy: Registry Whitelisting"},{"location":"add-ons/prometheus-grafana/","text":"Add-On: Prometheus & Grafana \u00b6 Deploy \u00b6 By running deploy-prometheus-grafana.sh you'll get a fully functional and preconfigured Prometheus and Grafana instance on the playground. The following additional scrapers are configured: api-collector Falco smartcheck-metrics Access \u00b6 Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to smartcheck. Linux By default, the Prometheus UI is on port 8081, Grafana on port 8080. Example: Prometheus UI on: http://192.168.1.121:8081 Grafana UI on: http://192.168.1.121:8080 w/ admin/trendmicro Cloud9 See: Access Smart Check (Cloud9) , but use the proxy_listen_port s configured in your config.yaml (default: 8080 (grafana) and 8081 (prometheus)) and choose Source Anywhere. Don't forget to check your inbound rules to allow these ports. Alternatively, you can get the configured port for the service with cat services . Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s). Example: Grafana: <http://YOUR-CLOUD9-PUBLIC-IP:8080> Prometheus: <http://YOUR-CLOUD9-PUBLIC-IP:8081>","title":"Prometheus & Grafana"},{"location":"add-ons/prometheus-grafana/#add-on-prometheus-grafana","text":"","title":"Add-On: Prometheus &amp; Grafana"},{"location":"add-ons/prometheus-grafana/#deploy","text":"By running deploy-prometheus-grafana.sh you'll get a fully functional and preconfigured Prometheus and Grafana instance on the playground. The following additional scrapers are configured: api-collector Falco smartcheck-metrics","title":"Deploy"},{"location":"add-ons/prometheus-grafana/#access","text":"Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to smartcheck. Linux By default, the Prometheus UI is on port 8081, Grafana on port 8080. Example: Prometheus UI on: http://192.168.1.121:8081 Grafana UI on: http://192.168.1.121:8080 w/ admin/trendmicro Cloud9 See: Access Smart Check (Cloud9) , but use the proxy_listen_port s configured in your config.yaml (default: 8080 (grafana) and 8081 (prometheus)) and choose Source Anywhere. Don't forget to check your inbound rules to allow these ports. Alternatively, you can get the configured port for the service with cat services . Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s). Example: Grafana: <http://YOUR-CLOUD9-PUBLIC-IP:8080> Prometheus: <http://YOUR-CLOUD9-PUBLIC-IP:8081>","title":"Access"},{"location":"add-ons/registry/","text":"Add-On: Registry \u00b6 Deploy \u00b6 To deploy the registry run: deploy-registry.sh Access \u00b6 Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to the registry. Linux Example: Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin Cloud9 A file called services is either created or updated with the link and the credentials to connect to the registry. Example: Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin","title":"Registry"},{"location":"add-ons/registry/#add-on-registry","text":"","title":"Add-On: Registry"},{"location":"add-ons/registry/#deploy","text":"To deploy the registry run: deploy-registry.sh","title":"Deploy"},{"location":"add-ons/registry/#access","text":"Follow the steps for your platform below. A file called services is either created or updated with the link and the credentials to connect to the registry. Linux Example: Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin Cloud9 A file called services is either created or updated with the link and the credentials to connect to the registry. Example: Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin","title":"Access"},{"location":"add-ons/trivy/","text":"Add-On: Trivy \u00b6 Deploy \u00b6 Fundamentally, Trivy gathers security data from various Kubernetes security tools into Kubernetes Custom Resource Definitions (CRD). These extend the Kubernetes APIs so that users can manage and access security reports through the Kubernetes interfaces, like kubectl. To deploy it, run deploy-trivy.sh Usage \u00b6 Workload Scanning # Vulnerability audit kubectl get vulnerabilityreports --all-namespaces -o wide # Configuration audit kubectl get configauditreports --all-namespaces -o wide Inspect any of the reports run something like this kubectl describe vulnerabilityreport -n kube-system daemonset-kube-proxy","title":"Trivy"},{"location":"add-ons/trivy/#add-on-trivy","text":"","title":"Add-On: Trivy"},{"location":"add-ons/trivy/#deploy","text":"Fundamentally, Trivy gathers security data from various Kubernetes security tools into Kubernetes Custom Resource Definitions (CRD). These extend the Kubernetes APIs so that users can manage and access security reports through the Kubernetes interfaces, like kubectl. To deploy it, run deploy-trivy.sh","title":"Deploy"},{"location":"add-ons/trivy/#usage","text":"Workload Scanning # Vulnerability audit kubectl get vulnerabilityreports --all-namespaces -o wide # Configuration audit kubectl get configauditreports --all-namespaces -o wide Inspect any of the reports run something like this kubectl describe vulnerabilityreport -n kube-system daemonset-kube-proxy","title":"Usage"},{"location":"experimenting/deploy-from-private-registry/","text":"Deploy Cloud One Container Security from a Private Registry \u00b6 Tools \u00b6 Used tools: Docker yq, awk, helm Get yq curl -L https://github.com/mikefarah/yq/releases/download/v4.24.2/yq_linux_amd64.tar.gz -o yq_linux_amd64.tar.gz tar xfvz yq_linux_amd64.tar.gz sudo cp yq_linux_amd64 /usr/local/bin/yq Login to the Registries \u00b6 export REGISTRY = 172 .250.255.1:5000 export USERNAME = admin export PASSWORD = trendmicro # Login to Docker Registry docker login # Login to private Registry echo ${ PASSWORD } | docker login https:// ${ REGISTRY } --username ${ USERNAME } --password-stdin Container Security - Pull,Tag, & Push \u00b6 # Enumerate the Images curl -L https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz -o master-cs.tar.gz tar xfvz master-cs.tar.gz export TAG = $( yq '.images.defaults.tag' cloudone-container-security-helm-master/values.yaml ) echo ${ TAG } # Pull Smart Check images from Dockerhub. awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker pull {} # Tag the images with your target registry information, making sure to preserve the original image name. awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker tag {} ${ REGISTRY } / {} # Push the images to the private registry awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker push ${ REGISTRY } / {} # Create image pull secret kubectl create secret docker-registry regcred \\ --docker-server = ${ REGISTRY } \\ --docker-username = ${ USERNAME } \\ --docker-password = ${ PASSWORD } \\ --namespace = container-security Update Container Securities overrides.yaml to override the default source registry with your private registry: ... images : defaults : registry : [ REGISTRY ] tag : [ TAG ] imagePullSecret : regcred Example: ... images : defaults : registry : 172.250.255.1:5000 tag : 2.2.9 imagePullSecret : regcred Deploy Container Security. helm install \\ container-security \\ --values $PGPATH /overrides.yaml \\ --namespace trendmicro-system \\ --install \\ https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz Smart Check - Pull,Tag, & Push \u00b6 # Enumerate the Images curl -L https://github.com/deep-security/smartcheck-helm/archive/master.tar.gz -o master-dssc.tar.gz tar xfvz master-dssc.tar.gz export TAG = $( yq '.images.defaults.tag' smartcheck-helm-master/values.yaml ) echo ${ TAG } # Pull Smart Check images from Dockerhub. awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker pull {} # Tag the images with your target registry information, making sure to preserve the original image name. awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker tag {} ${ REGISTRY } / {} # Push the images to the private registry awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker push ${ REGISTRY } / {} # Create image pull secret kubectl create secret docker-registry regcred \\ --docker-server = ${ REGISTRY } \\ --docker-username = ${ USERNAME } \\ --docker-password = ${ PASSWORD } \\ --namespace = smartcheck Update Container Securities overrides.yaml to override the default source registry with your private registry: ... images : defaults : registry : [ REGISTRY ] tag : [ TAG ] imagePullSecret : regcred Example: ... images : defaults : registry : 172.250.255.1:5000 tag : 1.2.76 imagePullSecret : regcred Deploy Smart Check helm install \\ smartcheck \\ --values $PGPATH /overrides.yaml \\ --namespace smartcheck \\ --install \\ https://github.com/deep-security/smartcheck-helm/archive/master.tar.gz","title":"Deploy from private registries"},{"location":"experimenting/deploy-from-private-registry/#deploy-cloud-one-container-security-from-a-private-registry","text":"","title":"Deploy Cloud One Container Security from a Private Registry"},{"location":"experimenting/deploy-from-private-registry/#tools","text":"Used tools: Docker yq, awk, helm Get yq curl -L https://github.com/mikefarah/yq/releases/download/v4.24.2/yq_linux_amd64.tar.gz -o yq_linux_amd64.tar.gz tar xfvz yq_linux_amd64.tar.gz sudo cp yq_linux_amd64 /usr/local/bin/yq","title":"Tools"},{"location":"experimenting/deploy-from-private-registry/#login-to-the-registries","text":"export REGISTRY = 172 .250.255.1:5000 export USERNAME = admin export PASSWORD = trendmicro # Login to Docker Registry docker login # Login to private Registry echo ${ PASSWORD } | docker login https:// ${ REGISTRY } --username ${ USERNAME } --password-stdin","title":"Login to the Registries"},{"location":"experimenting/deploy-from-private-registry/#container-security-pulltag-push","text":"# Enumerate the Images curl -L https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz -o master-cs.tar.gz tar xfvz master-cs.tar.gz export TAG = $( yq '.images.defaults.tag' cloudone-container-security-helm-master/values.yaml ) echo ${ TAG } # Pull Smart Check images from Dockerhub. awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker pull {} # Tag the images with your target registry information, making sure to preserve the original image name. awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker tag {} ${ REGISTRY } / {} # Push the images to the private registry awk -v tag = $TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\ cloudone-container-security-helm-master/values.yaml | xargs -I {} docker push ${ REGISTRY } / {} # Create image pull secret kubectl create secret docker-registry regcred \\ --docker-server = ${ REGISTRY } \\ --docker-username = ${ USERNAME } \\ --docker-password = ${ PASSWORD } \\ --namespace = container-security Update Container Securities overrides.yaml to override the default source registry with your private registry: ... images : defaults : registry : [ REGISTRY ] tag : [ TAG ] imagePullSecret : regcred Example: ... images : defaults : registry : 172.250.255.1:5000 tag : 2.2.9 imagePullSecret : regcred Deploy Container Security. helm install \\ container-security \\ --values $PGPATH /overrides.yaml \\ --namespace trendmicro-system \\ --install \\ https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz","title":"Container Security - Pull,Tag, &amp; Push"},{"location":"experimenting/deploy-from-private-registry/#smart-check-pulltag-push","text":"# Enumerate the Images curl -L https://github.com/deep-security/smartcheck-helm/archive/master.tar.gz -o master-dssc.tar.gz tar xfvz master-dssc.tar.gz export TAG = $( yq '.images.defaults.tag' smartcheck-helm-master/values.yaml ) echo ${ TAG } # Pull Smart Check images from Dockerhub. awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker pull {} # Tag the images with your target registry information, making sure to preserve the original image name. awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker tag {} ${ REGISTRY } / {} # Push the images to the private registry awk -v tag = $TAG '$1 == \"repository:\" {printf \"deepsecurity/%s:%s\\n\",$2,tag;}' \\ smartcheck-helm-master/values.yaml | xargs -I {} docker push ${ REGISTRY } / {} # Create image pull secret kubectl create secret docker-registry regcred \\ --docker-server = ${ REGISTRY } \\ --docker-username = ${ USERNAME } \\ --docker-password = ${ PASSWORD } \\ --namespace = smartcheck Update Container Securities overrides.yaml to override the default source registry with your private registry: ... images : defaults : registry : [ REGISTRY ] tag : [ TAG ] imagePullSecret : regcred Example: ... images : defaults : registry : 172.250.255.1:5000 tag : 1.2.76 imagePullSecret : regcred Deploy Smart Check helm install \\ smartcheck \\ --values $PGPATH /overrides.yaml \\ --namespace smartcheck \\ --install \\ https://github.com/deep-security/smartcheck-helm/archive/master.tar.gz","title":"Smart Check - Pull,Tag, &amp; Push"},{"location":"experimenting/migrate/","text":"Migrate \u00b6 This tries to solve the challenge to migrate workload of an existing cluster using public image registries to a trusted, private one (without breaking the services). To try it, being in the playground directory run migrate/save-cluster.sh This scripts dumps the full cluster to json files separated by namespace. The namespaces kube-system , registry and default are currently excluded. Effectively, this is a backup of your cluster including ConfigMaps and Secrets etc. which you can deploy on a different cluster easily ( kubectl create -f xxx.json ) To migrate the images currently in use run migrate/migrate-images.sh This second script updates the saved manifests in regards the image location to point them to the private registry. If the image has a digest within it's name it is stripped. The image get's then pulled from the public repo and pushed to the internal one. This is followed by an image scan (currently by Smart Check) and the redeployment. Note: at the time of writing the only supported private registry is the internal one.","title":"Migrate to a Private Registry"},{"location":"experimenting/migrate/#migrate","text":"This tries to solve the challenge to migrate workload of an existing cluster using public image registries to a trusted, private one (without breaking the services). To try it, being in the playground directory run migrate/save-cluster.sh This scripts dumps the full cluster to json files separated by namespace. The namespaces kube-system , registry and default are currently excluded. Effectively, this is a backup of your cluster including ConfigMaps and Secrets etc. which you can deploy on a different cluster easily ( kubectl create -f xxx.json ) To migrate the images currently in use run migrate/migrate-images.sh This second script updates the saved manifests in regards the image location to point them to the private registry. If the image has a digest within it's name it is stripped. The image get's then pulled from the public repo and pushed to the internal one. This is followed by an image scan (currently by Smart Check) and the redeployment. Note: at the time of writing the only supported private registry is the internal one.","title":"Migrate"},{"location":"experimenting/pipelining-on-aws/","text":"Pipelining on AWS \u00b6 Requirements & Preparations \u00b6 Pipelining on AWS requires an EKS cluster, of course. So this pipeline does not work with any other Playground variant. At a minimum, the following steps needs to be executed prior running the deploy-pipeline-aws.sh -script: # Playground needs to be bootstrapped # If running on Cloud9 # Don't forget to turn off managed credentials in your Cloud9 tools/cloud9-resize.sh tools/cloud9-instance-role.sh # If running on Linux/Mac: aws configure # Build EKS cluster clusters/rapid-eks.sh # Deploy Smart Check ./deploy-smartcheck.sh Deployment \u00b6 Run ./deploy-pipeline-aws.sh This script automates the following: Creation of a role for CodeBuild - In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster. In this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl. Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster. Once the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role. We're going to create the AWS CodePipeline using AWS CloudFormation which defines all used resources for our pipeline. In our case this includes ECR, S3, CodeBuild, CodePipeline, CodeCommit, ServiceRoles, and Smart Check. Next, we're adding a remote repository in AWS CodeCommit which our pipeline will use. At the time of writing the script clones my c1-app-sec-uploader . While doing this we create the kubernetes manifest for our app. Finally, we push our code to the CodeCommit repo which will trigger the pipeline run. The pipeline builds the container image, pushes it to ECR, scans the image with Smart Check and finally deploys it to EKS. The deployment obviously can fail if you're running Cloud One Container Security on the cluster, since the image will contain vulnerabilities. So it just depends on you and your defined policy. If everything works you'll have a running uploader demo on your cluster. Query the URL by kubectl -n default get svc and upload some malware, if you want Further Reading \u00b6 AWS EKS Cluster Authentication: https://docs.aws.amazon.com/eks/latest/userguide/cluster-auth.html Tear Down \u00b6 To tear down the pipeline including CodeCommit, Roles etc. simply run the auto-generated script pipeline-aws-down.sh","title":"Pipelining on AWS"},{"location":"experimenting/pipelining-on-aws/#pipelining-on-aws","text":"","title":"Pipelining on AWS"},{"location":"experimenting/pipelining-on-aws/#requirements-preparations","text":"Pipelining on AWS requires an EKS cluster, of course. So this pipeline does not work with any other Playground variant. At a minimum, the following steps needs to be executed prior running the deploy-pipeline-aws.sh -script: # Playground needs to be bootstrapped # If running on Cloud9 # Don't forget to turn off managed credentials in your Cloud9 tools/cloud9-resize.sh tools/cloud9-instance-role.sh # If running on Linux/Mac: aws configure # Build EKS cluster clusters/rapid-eks.sh # Deploy Smart Check ./deploy-smartcheck.sh","title":"Requirements &amp; Preparations"},{"location":"experimenting/pipelining-on-aws/#deployment","text":"Run ./deploy-pipeline-aws.sh This script automates the following: Creation of a role for CodeBuild - In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster. In this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl. Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster. Once the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role. We're going to create the AWS CodePipeline using AWS CloudFormation which defines all used resources for our pipeline. In our case this includes ECR, S3, CodeBuild, CodePipeline, CodeCommit, ServiceRoles, and Smart Check. Next, we're adding a remote repository in AWS CodeCommit which our pipeline will use. At the time of writing the script clones my c1-app-sec-uploader . While doing this we create the kubernetes manifest for our app. Finally, we push our code to the CodeCommit repo which will trigger the pipeline run. The pipeline builds the container image, pushes it to ECR, scans the image with Smart Check and finally deploys it to EKS. The deployment obviously can fail if you're running Cloud One Container Security on the cluster, since the image will contain vulnerabilities. So it just depends on you and your defined policy. If everything works you'll have a running uploader demo on your cluster. Query the URL by kubectl -n default get svc and upload some malware, if you want","title":"Deployment"},{"location":"experimenting/pipelining-on-aws/#further-reading","text":"AWS EKS Cluster Authentication: https://docs.aws.amazon.com/eks/latest/userguide/cluster-auth.html","title":"Further Reading"},{"location":"experimenting/pipelining-on-aws/#tear-down","text":"To tear down the pipeline including CodeCommit, Roles etc. simply run the auto-generated script pipeline-aws-down.sh","title":"Tear Down"},{"location":"getting-started/configuration-simplicity/","text":"Getting Started Configuration \u00b6 From within the main menu choose Edit Configuration Typically you don't need to change anything here besides setting your api-key and region for Cloud One. If you intent to run multiple clusters (e.g. a local and a GKE), adapt the cluster_name and the policy_name . { \"cluster_name\" : \"playground\" , \"services\" : [ ... { \"name\" : \"cloudone\" , \"region\" : \"YOUR CLOUD ONE REGION HERE\" , \"instance\" : \"cloudone\" , \"api_key\" : \"YOUR CLOUD ONE API KEY HERE\" } ... { \"name\" : \"container_security\" , \"policy_name\" : \"relaxed_playground\" , \"namespace\" : \"trendmicro-system\" }, ... ] } Other service sections you might want to adjust to your needs: Pipelining By default, the configuration is pointing to the Uploader example from my GitHub. If you want to use a different app change the three github values accordingly. { \"cluster_name\" : \"playground\" , \"services\" : [ ... { \"name\" : \"pipeline\" , \"github_username\" : \"mawinkler\" , \"github_email\" : \"winkler.info@icloud.com\" , \"github_project\" : \"c1-app-sec-uploader\" , \"docker_username\" : \"YOUR USERNAME HERE\" , \"docker_password\" : \"YOUR PASSWORD HERE\" , \"appsec_key\" : \"YOUR KEY HERE\" , \"appsec_secret\" : \"YOUR SECRET HERE\" }, ... ] }","title":"Configuration"},{"location":"getting-started/configuration-simplicity/#getting-started-configuration","text":"From within the main menu choose Edit Configuration Typically you don't need to change anything here besides setting your api-key and region for Cloud One. If you intent to run multiple clusters (e.g. a local and a GKE), adapt the cluster_name and the policy_name . { \"cluster_name\" : \"playground\" , \"services\" : [ ... { \"name\" : \"cloudone\" , \"region\" : \"YOUR CLOUD ONE REGION HERE\" , \"instance\" : \"cloudone\" , \"api_key\" : \"YOUR CLOUD ONE API KEY HERE\" } ... { \"name\" : \"container_security\" , \"policy_name\" : \"relaxed_playground\" , \"namespace\" : \"trendmicro-system\" }, ... ] } Other service sections you might want to adjust to your needs: Pipelining By default, the configuration is pointing to the Uploader example from my GitHub. If you want to use a different app change the three github values accordingly. { \"cluster_name\" : \"playground\" , \"services\" : [ ... { \"name\" : \"pipeline\" , \"github_username\" : \"mawinkler\" , \"github_email\" : \"winkler.info@icloud.com\" , \"github_project\" : \"c1-app-sec-uploader\" , \"docker_username\" : \"YOUR USERNAME HERE\" , \"docker_password\" : \"YOUR PASSWORD HERE\" , \"appsec_key\" : \"YOUR KEY HERE\" , \"appsec_secret\" : \"YOUR SECRET HERE\" }, ... ] }","title":"Getting Started Configuration"},{"location":"getting-started/kind-simplicity/","text":"Getting Started w/ the built in Cluster \u00b6 Choose the platform documentation Ubuntu \u00b6 Follow this chapter if... you're using the Playground on a Ubuntu machine (not Cloud9) and are going to use the built in cluster Test if sudo requires a password by running sudo ls /etc . If you don't get a password prompt you're fine, otherwise run. sudo visudo -f /etc/sudoers.d/custom-users Add the following line: <YOUR USER NAME> ALL =( ALL ) NOPASSWD:ALL Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Finally, create your local cluster by choosing Create Cluster... --> Local Cluster . Cloud9 \u00b6 Follow this chapter if... you're using the Playground on a AWS Cloud9 environment and are going to use the built in cluster Follow the steps below to create a Cloud9 suitable for the Playground. Point your browser to AWS Choose your default AWS region in the top right Go to the Cloud9 service Select [Create Cloud9 environment] Name it as you like Choose [t3.xlarge] for instance type and Ubuntu 18.04 LTS as the platform For the rest take all default values and click [Create environment] Update IAM Settings for the Workspace Click the gear icon (in top right corner), or click to open a new tab and choose [Open Preferences] Select AWS SETTINGS Turn OFF [AWS managed temporary credentials] Close the Preferences tab Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap . You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance. If you forgot to disable AWS managed temporary credentials you will asked to do it again. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Finally, create your local cluster by choosing Create Cluster... --> Local Cluster .","title":"Built-in Cluster"},{"location":"getting-started/kind-simplicity/#getting-started-w-the-built-in-cluster","text":"Choose the platform documentation","title":"Getting Started w/ the built in Cluster"},{"location":"getting-started/kind-simplicity/#ubuntu","text":"Follow this chapter if... you're using the Playground on a Ubuntu machine (not Cloud9) and are going to use the built in cluster Test if sudo requires a password by running sudo ls /etc . If you don't get a password prompt you're fine, otherwise run. sudo visudo -f /etc/sudoers.d/custom-users Add the following line: <YOUR USER NAME> ALL =( ALL ) NOPASSWD:ALL Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Finally, create your local cluster by choosing Create Cluster... --> Local Cluster .","title":"Ubuntu"},{"location":"getting-started/kind-simplicity/#cloud9","text":"Follow this chapter if... you're using the Playground on a AWS Cloud9 environment and are going to use the built in cluster Follow the steps below to create a Cloud9 suitable for the Playground. Point your browser to AWS Choose your default AWS region in the top right Go to the Cloud9 service Select [Create Cloud9 environment] Name it as you like Choose [t3.xlarge] for instance type and Ubuntu 18.04 LTS as the platform For the rest take all default values and click [Create environment] Update IAM Settings for the Workspace Click the gear icon (in top right corner), or click to open a new tab and choose [Open Preferences] Select AWS SETTINGS Turn OFF [AWS managed temporary credentials] Close the Preferences tab Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap . You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance. If you forgot to disable AWS managed temporary credentials you will asked to do it again. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Finally, create your local cluster by choosing Create Cluster... --> Local Cluster .","title":"Cloud9"},{"location":"getting-started/managed-simplicity/","text":"Getting Started w/ managed Clusters \u00b6 Choose the platform documentation Ubuntu \u00b6 Follow this chapter if... you're using the Playground on a Ubuntu machine and are going to use either EKS, AKS or GKE Test if sudo requires a password by running sudo ls /etc . If you don't get a password prompt you're fine, otherwise run. sudo visudo -f /etc/sudoers.d/custom-users Add the following line: <YOUR USER NAME> ALL =( ALL ) NOPASSWD:ALL Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Now, choose the option Install/Update CLI... --> AWS CLI , Azure CLI , or GCP CLI . Next, you need to ensure that your CLI is authenticated. Do this via the option Authenticate to CSP... and follow the instructions. Finally, create your cluster by choosing Create Cluster... --> EKS , AKS , or GKE . Cloud9 \u00b6 Follow this chapter if... you're using the Playground on a AWS Cloud9 environment and are going to use EKS as the cluster Follow the steps below to create a Cloud9 suitable for the Playground. Point your browser to AWS Choose your default AWS region in the top right Go to the Cloud9 service Select [Create Cloud9 environment] Name it as you like Choose [t3.medium] for instance type and Ubuntu 18.04 LTS as the platform For the rest take all default values and click [Create environment] Update IAM Settings for the Workspace Click the gear icon (in top right corner), or click to open a new tab and choose [Open Preferences] Select AWS SETTINGS Turn OFF [AWS managed temporary credentials] Close the Preferences tab Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap . You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance. If you forgot to disable AWS managed temporary credentials you will asked to do it again. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Now, choose the option Install/Update CLI... --> AWS CLI , Azure CLI , or GCP CLI . If you're going to use EKS, you don't need to do anything additionally before creating the EKS cluster. In the case of AKS or GKE, you need to authenticate to Azure or GCP first. To authenticate to your CSP choose the option Authenticate to CSP... and follow the instructions. Finally, create your cluster by choosing Create Cluster... --> EKS , AKS , or GKE .","title":"Managed Clusters"},{"location":"getting-started/managed-simplicity/#getting-started-w-managed-clusters","text":"Choose the platform documentation","title":"Getting Started w/ managed Clusters"},{"location":"getting-started/managed-simplicity/#ubuntu","text":"Follow this chapter if... you're using the Playground on a Ubuntu machine and are going to use either EKS, AKS or GKE Test if sudo requires a password by running sudo ls /etc . If you don't get a password prompt you're fine, otherwise run. sudo visudo -f /etc/sudoers.d/custom-users Add the following line: <YOUR USER NAME> ALL =( ALL ) NOPASSWD:ALL Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Now, choose the option Install/Update CLI... --> AWS CLI , Azure CLI , or GCP CLI . Next, you need to ensure that your CLI is authenticated. Do this via the option Authenticate to CSP... and follow the instructions. Finally, create your cluster by choosing Create Cluster... --> EKS , AKS , or GKE .","title":"Ubuntu"},{"location":"getting-started/managed-simplicity/#cloud9","text":"Follow this chapter if... you're using the Playground on a AWS Cloud9 environment and are going to use EKS as the cluster Follow the steps below to create a Cloud9 suitable for the Playground. Point your browser to AWS Choose your default AWS region in the top right Go to the Cloud9 service Select [Create Cloud9 environment] Name it as you like Choose [t3.medium] for instance type and Ubuntu 18.04 LTS as the platform For the rest take all default values and click [Create environment] Update IAM Settings for the Workspace Click the gear icon (in top right corner), or click to open a new tab and choose [Open Preferences] Select AWS SETTINGS Turn OFF [AWS managed temporary credentials] Close the Preferences tab Now, run the Playground curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash Choose Bootstrap . You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance. If you forgot to disable AWS managed temporary credentials you will asked to do it again. After the bootstrapping has finished exit the menu and the terminal. Then create a new terminal. To restart the menu execute playground from anywhere in your terminal. Now, choose the option Install/Update CLI... --> AWS CLI , Azure CLI , or GCP CLI . If you're going to use EKS, you don't need to do anything additionally before creating the EKS cluster. In the case of AKS or GKE, you need to authenticate to Azure or GCP first. To authenticate to your CSP choose the option Authenticate to CSP... and follow the instructions. Finally, create your cluster by choosing Create Cluster... --> EKS , AKS , or GKE .","title":"Cloud9"},{"location":"play/scripts/","text":"Demo Scripts \u00b6 The Playground supports automated scripts to demonstrate functionalities of deployments. Currently, there are two scripts available showing some capabilities of Cloud One Container Security. To run them, ensure to have an EKS cluster up and running and have Smart Check and Container Security deployed. After configuring the policy and rule set as shown below, you can run the demos with # Deployment Control Demo ./demos/demo-c1cs-dc.sh # Runtime Security Demo ./demos/demo-c1cs-rt.sh Deployment Control Demo \u00b6 Storyline: A developer wants to try out a new nginx image but fails since the image has critical vulnerabilities, he tries to deploy from docker hub etc. Lastly he tries to attach to the pod, which is prevented by Container Security. To prepare for the demo verify that the cluster policy is set as shown below: Pod properties uncheck - containers that run as root Block - containers that run in the host network namespace Block - containers that run in the host IPC namespace Block - containers that run in the host PID namespace Container properties Block - containers that are permitted to run as root Block - privileged containers Block - containers with privilege escalation rights Block - containers that can write to the root filesystem Image properties Block - images from registries with names that DO NOT EQUAL REGISTRY:PORT uncheck - images with names that Log - images with tags that EQUAL latest uncheck - images with image paths that Scan Results Block - images that are not scanned Block - images with malware Log - images with content findings whose severity is CRITICAL OR HIGHER Log - images with checklists whose severity is CRITICAL OR HIGHER Log - images with vulnerabilities whose severity is CRITICAL OR HIGHER Block - images with vulnerabilities whose CVSS attack vector is NETWORK and whose severity is HIGH OR HIGHER Block - images with vulnerabilities whose CVSS attack complexity is LOW and whose severity is HIGH OR HIGHER Block - images with vulnerabilities whose CVSS availability impact is HIGH and whose severity is HIGH OR HIGHER Log - images with a negative PCI-DSS checklist result with severity CRITICAL OR HIGHER Kubectl Access Block - attempts to execute in/attach to a container Log - attempts to establish port-forward on a container Most of it should already configured by the deploy-container-security.sh script. Run the demo being in the playground directory with ./demos/demo-c1cs-dc.sh Runtime Security Demo \u00b6 Storyline: A kubernetes admin newbie executes some information gathering about the kubernetes cluster from within a running pod. Finally, he gets kicked by Container Security because of the kubectl usage. To successfully run the runtime demo you need adjust the aboves policy slightly. Change: Kubectl Access Log - attempts to execute in/attach to a container Exceptions Allow images with paths that equal docker.io/mawinkler/ubuntu:latest Additionally, set the runtime rule (T1543)Launch Package Management Process in Container to Log . Normally you'll find that rule in the *_error ruleset. Run the demo being in the playground directory with ./demos/demo-c1cs-rt.sh The demo starts locally on your system, but creates a pod in the default namespace of your cluster using a slightly pimped ubuntu image which is pulled from my docker hub account. The main demo runs within that pod on the cluster, not on your local machine. The Dockerfile for this image is in ./demos/pod/Dockerfile for you to verify, but you do not need to build it yourself.","title":"Demo Scripts"},{"location":"play/scripts/#demo-scripts","text":"The Playground supports automated scripts to demonstrate functionalities of deployments. Currently, there are two scripts available showing some capabilities of Cloud One Container Security. To run them, ensure to have an EKS cluster up and running and have Smart Check and Container Security deployed. After configuring the policy and rule set as shown below, you can run the demos with # Deployment Control Demo ./demos/demo-c1cs-dc.sh # Runtime Security Demo ./demos/demo-c1cs-rt.sh","title":"Demo Scripts"},{"location":"play/scripts/#deployment-control-demo","text":"Storyline: A developer wants to try out a new nginx image but fails since the image has critical vulnerabilities, he tries to deploy from docker hub etc. Lastly he tries to attach to the pod, which is prevented by Container Security. To prepare for the demo verify that the cluster policy is set as shown below: Pod properties uncheck - containers that run as root Block - containers that run in the host network namespace Block - containers that run in the host IPC namespace Block - containers that run in the host PID namespace Container properties Block - containers that are permitted to run as root Block - privileged containers Block - containers with privilege escalation rights Block - containers that can write to the root filesystem Image properties Block - images from registries with names that DO NOT EQUAL REGISTRY:PORT uncheck - images with names that Log - images with tags that EQUAL latest uncheck - images with image paths that Scan Results Block - images that are not scanned Block - images with malware Log - images with content findings whose severity is CRITICAL OR HIGHER Log - images with checklists whose severity is CRITICAL OR HIGHER Log - images with vulnerabilities whose severity is CRITICAL OR HIGHER Block - images with vulnerabilities whose CVSS attack vector is NETWORK and whose severity is HIGH OR HIGHER Block - images with vulnerabilities whose CVSS attack complexity is LOW and whose severity is HIGH OR HIGHER Block - images with vulnerabilities whose CVSS availability impact is HIGH and whose severity is HIGH OR HIGHER Log - images with a negative PCI-DSS checklist result with severity CRITICAL OR HIGHER Kubectl Access Block - attempts to execute in/attach to a container Log - attempts to establish port-forward on a container Most of it should already configured by the deploy-container-security.sh script. Run the demo being in the playground directory with ./demos/demo-c1cs-dc.sh","title":"Deployment Control Demo"},{"location":"play/scripts/#runtime-security-demo","text":"Storyline: A kubernetes admin newbie executes some information gathering about the kubernetes cluster from within a running pod. Finally, he gets kicked by Container Security because of the kubectl usage. To successfully run the runtime demo you need adjust the aboves policy slightly. Change: Kubectl Access Log - attempts to execute in/attach to a container Exceptions Allow images with paths that equal docker.io/mawinkler/ubuntu:latest Additionally, set the runtime rule (T1543)Launch Package Management Process in Container to Log . Normally you'll find that rule in the *_error ruleset. Run the demo being in the playground directory with ./demos/demo-c1cs-rt.sh The demo starts locally on your system, but creates a pod in the default namespace of your cluster using a slightly pimped ubuntu image which is pulled from my docker hub account. The main demo runs within that pod on the cluster, not on your local machine. The Dockerfile for this image is in ./demos/pod/Dockerfile for you to verify, but you do not need to build it yourself.","title":"Runtime Security Demo"},{"location":"play/tools/","text":"Tools & Scripts \u00b6 Disclaimer \u00b6 All the scripts and tools described here are at a proof-of-concept level. Some finetuning for production use is advised. Additionally, they might not be officially supported by the Cloud One solutions :-). Cloud One Sentry \u00b6 Within the tools directory are some scripts for Sentry: sentry-get-reports - Downloads all Sentry reports generated within the last 24hs to your local directory sentry-trigger-ebs-scan - Trigger a full scan for a given EC2 instance with one or more EBS volumes attached sentry-trigger-ecr-scan - Trigger a full scan for a given ECR repo sentry-trigger-ebs-scan - Trigger a full scan for a given Lambda sentry-remove-snapshots - Delete snapshots created by sentry-trigger-ebs-scan Script sentry-get-reports \u00b6 The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the reports. Example calls: # Help sentry-get-reports help Usage: [ REGION = <aws-region> ] sentry-get-reports Example: REGION = eu-central-1 sentry-get-reports # Get the reports from the current region sentry-get-reports # Get the reports from a region other than your current region REGION = us-east-1 sentry-get-reports Example result: sentry-reports-2023-02-13_15-42-54 \u251c\u2500\u2500 aws-ebs-volumes \u2502 \u251c\u2500\u2500 i-0076dab31026905f5.json \u2502 \u251c\u2500\u2500 i-03c03a975195bf87b.json \u2502 \u2514\u2500\u2500 i-08a345a13318f7db0.json \u251c\u2500\u2500 aws-ecr-images \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_busybox:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_hello-world:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_mawinkler_evil:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_nginx:1.21.6.json \u2502 \u2514\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_serverless-webhook:latest.json \u2514\u2500\u2500 aws-lambda-functions \u251c\u2500\u2500 CloudOneWorkloadSecurityUS1SNSPublish.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-CreateLambdaAliasLambda-UPdmsJoha1xM.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-GetLambdaLastConfig-Uj2V8nohPYhb.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-ScannerLambda-mEoyirF86l1J.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-ScannerDeadLetterLambda-ftXbhuqoFN2b.json \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-gq0psizTvl7R.json \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-MyKEV22XwkY6.json \u251c\u2500\u2500 SecurityHubStack-CreateIntegrationFunctionB363DF0B-Jz2BcKzj5NvD.json \u251c\u2500\u2500 serverless-webhook.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanAM-LC9XncBvNApb.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanIM-xImAvOBHiHRx.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a3-sideScanReport-t3DwsQNXYVoj.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-ScanInvokerFunction-NnmdRDpmPbH1.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-sideScanParseVolume-8Ya9840jWIuv.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a2-LambdaUpdaterFunction-HZpcwfJXwNTb.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a-SendScanReportFunction-2X3KHz69ulrV.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8-ScanSQSConsumerFunction-Y24t3VpgJ4eG.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-deleteEbsSnapshotFunctio-YsSJWo4pwcOP.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-ecrResourceProviderFunct-sL0O6KdHpsWK.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-lambdaResourceProviderFu-VdfrMST0mtXQ.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-snapshotDistributionFunc-s1faGUTYT8sd.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-SnapshotProviderFunction-07HwDBE41qO0.json \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-IMPZcIjLGFTj.json \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-R8mg71T8Qj7V.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-bSaQH7juC9ZV.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-zkG5ey8gSFje.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-5IMogaWAeCBy.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-B5cdL2YrPbk0.json \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-a2bmnXZRLzxw.json \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-C48dmTDLXKBK.json \u2514\u2500\u2500 Storage-TM-FileStorageSecurity_ScanSendEmail.json Script sentry-trigger-ebs-scan \u00b6 The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-ebs-scan help Please specify at least the ec2 instance to be scanned. Usage: INSTANCE = <instance-id> [ REGION = <aws-region> ] [ USERNAME = <username-tag> ] sentry-trigger-ebs-scan Example: INSTANCE = i-0076dab31026905f5 sentry-trigger-ebs-scan If you specify a USERNAME the snapshot(s) will be tagged accordingly. This should ease identifying your own snapshots if using a shared account. Default username is cnctraining . # Trigger scan of EC2 instance i-0076dab31026905f5 existing in the current region INSTANCE = i-0076dab31026905f5 sentry-trigger-ebs-scan Example result: Using region eu-central-1 for user cnctraining State machine is arn:aws:states:eu-central-1:634503960501:stateMachine:ScannerStateMachine-pueSSKvfdN4K Instance volume ( s ) \\n vol-03b25f8105caf9f00 Snapshot snap-0f4cc9d1d8a094861 for volume vol-03b25f8105caf9f00 created { \"ScanID\" : \"634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\" , \"ResourceType\" : \"aws-ebs-volume\" , \"ResourceLocation\" : \"snap-0f4cc9d1d8a094861\" , \"ResourceRegion\" : \"eu-central-1\" , \"MetaData\" : { \"AWSAccountID\" : \"634503960501\" , \"SnapshotID\" : \"snap-0f4cc9d1d8a094861\" , \"VolumeID\" : \"vol-03b25f8105caf9f00\" , \"AttachedInstances\" : [ { \"InstanceID\" : \"i-0076dab31026905f5\" } ] } } { \"executionArn\" : \"arn:aws:states:eu-central-1:634503960501:execution:ScannerStateMachine-pueSSKvfdN4K:Manual-EBS-resource-634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\" , \"startDate\" : \"2023-03-02T14:36:00.763000+00:00\" } Script sentry-trigger-ecr-scan \u00b6 The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-ecr-scan help Please specify at least the ecr repository to be scanned. Usage: REPO = <repo-name> [ TAG = <image-tag ] [ REGION = <aws-region> ] sentry-trigger-ecr-scan Example: REPO = mawinkler/evil TAG = latest sentry-trigger-ecr-scan # Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region REPO = mawinkler/evil TAG = 0 .1 sentry-trigger-ecr-scan Script sentry-trigger-lambda-scan \u00b6 The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-lambda-scan help Please specify at least the lambda to be scanned. Usage: LAMBDA = <lambda-name> [ REGION = <aws-region> ] sentry-trigger-lambda-scan Example: LAMBDA = cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan # Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region LAMBDA = cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan Script sentry-remove-snapshots \u00b6 The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the reports. Example calls: # Help sentry-remove-snapshots help Usage: [ REGION = <aws-region> ] [ USERNAME = <username-tag> ] sentry-remove-snapshots Example: USERNAME = cnctraining sentry-remove-snapshots Example result: Using region eu-central-1 for user cnctraining Snapshot ( s ) to delete snap-0f4cc9d1d8a094861 snap-00f0472b795c00644 snap-025d77b5303a37b9d snap-0b357bbc2fef3edad snap-07ee34bf66221579d snap-0b4d1faead597e047 Deleting snapshot snap-0f4cc9d1d8a094861 Deleting snapshot snap-00f0472b795c00644 Deleting snapshot snap-025d77b5303a37b9d Deleting snapshot snap-0b357bbc2fef3edad Deleting snapshot snap-07ee34bf66221579d Deleting snapshot snap-0b4d1faead597e047 Repo C1 Sentry Reports to CloudWatch \u00b6 Here, I'm describing a simple way to easily get new Sentry reports to CloudWatch using Lambda. Repo Link Cloud One Container Security \u00b6 Scan-Image and Scan-Namespace with Smart Check \u00b6 The two scripts scan-image and scan-namespace do what you would expect. Running scan-image nginx:latest starts an asynchronous scan of the latest version of nginx. The scan will run on Smart Check, but you are immedeately back in the shell. To access the scan results either use the UI or API of Smart Check. ... { cri t ical : 6 , high : 39 , medium : 40 , low : 13 , ne gligible : 2 , u n k n ow n : 3 } The script scan-namespace.sh scans all used images within the current namespace. Eventually do a kubectl config set-context --current --namespace <NAMESPACE> beforehand to select the namespace to be scanned. Scan-Image with Artifact Scanning as a Service \u00b6 The script scan-assas do what you would expect, creating a scan request utilizing syft to create an SBOM and upload it to ASaaS for the vulnerability scan. Running scan-assas nginx:latest Should do the trick.","title":"Tools and Scripts"},{"location":"play/tools/#tools-scripts","text":"","title":"Tools &amp; Scripts"},{"location":"play/tools/#disclaimer","text":"All the scripts and tools described here are at a proof-of-concept level. Some finetuning for production use is advised. Additionally, they might not be officially supported by the Cloud One solutions :-).","title":"Disclaimer"},{"location":"play/tools/#cloud-one-sentry","text":"Within the tools directory are some scripts for Sentry: sentry-get-reports - Downloads all Sentry reports generated within the last 24hs to your local directory sentry-trigger-ebs-scan - Trigger a full scan for a given EC2 instance with one or more EBS volumes attached sentry-trigger-ecr-scan - Trigger a full scan for a given ECR repo sentry-trigger-ebs-scan - Trigger a full scan for a given Lambda sentry-remove-snapshots - Delete snapshots created by sentry-trigger-ebs-scan","title":"Cloud One Sentry"},{"location":"play/tools/#script-sentry-get-reports","text":"The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the reports. Example calls: # Help sentry-get-reports help Usage: [ REGION = <aws-region> ] sentry-get-reports Example: REGION = eu-central-1 sentry-get-reports # Get the reports from the current region sentry-get-reports # Get the reports from a region other than your current region REGION = us-east-1 sentry-get-reports Example result: sentry-reports-2023-02-13_15-42-54 \u251c\u2500\u2500 aws-ebs-volumes \u2502 \u251c\u2500\u2500 i-0076dab31026905f5.json \u2502 \u251c\u2500\u2500 i-03c03a975195bf87b.json \u2502 \u2514\u2500\u2500 i-08a345a13318f7db0.json \u251c\u2500\u2500 aws-ecr-images \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_busybox:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_hello-world:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_mawinkler_evil:latest.json \u2502 \u251c\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_nginx:1.21.6.json \u2502 \u2514\u2500\u2500 634503960501 .dkr.ecr.eu-central-1.amazonaws.com_serverless-webhook:latest.json \u2514\u2500\u2500 aws-lambda-functions \u251c\u2500\u2500 CloudOneWorkloadSecurityUS1SNSPublish.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-CreateLambdaAliasLambda-UPdmsJoha1xM.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-GetLambdaLastConfig-Uj2V8nohPYhb.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-ScannerLambda-mEoyirF86l1J.json \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-ScannerDeadLetterLambda-ftXbhuqoFN2b.json \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-gq0psizTvl7R.json \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-MyKEV22XwkY6.json \u251c\u2500\u2500 SecurityHubStack-CreateIntegrationFunctionB363DF0B-Jz2BcKzj5NvD.json \u251c\u2500\u2500 serverless-webhook.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanAM-LC9XncBvNApb.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanIM-xImAvOBHiHRx.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a3-sideScanReport-t3DwsQNXYVoj.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-ScanInvokerFunction-NnmdRDpmPbH1.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-sideScanParseVolume-8Ya9840jWIuv.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a2-LambdaUpdaterFunction-HZpcwfJXwNTb.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8a-SendScanReportFunction-2X3KHz69ulrV.json \u251c\u2500\u2500 StackSet-SentryStackSet-f8-ScanSQSConsumerFunction-Y24t3VpgJ4eG.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-deleteEbsSnapshotFunctio-YsSJWo4pwcOP.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-ecrResourceProviderFunct-sL0O6KdHpsWK.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-lambdaResourceProviderFu-VdfrMST0mtXQ.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-snapshotDistributionFunc-s1faGUTYT8sd.json \u251c\u2500\u2500 StackSet-SentryStackSet-f-SnapshotProviderFunction-07HwDBE41qO0.json \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-IMPZcIjLGFTj.json \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-R8mg71T8Qj7V.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-bSaQH7juC9ZV.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-zkG5ey8gSFje.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-5IMogaWAeCBy.json \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-B5cdL2YrPbk0.json \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-a2bmnXZRLzxw.json \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-C48dmTDLXKBK.json \u2514\u2500\u2500 Storage-TM-FileStorageSecurity_ScanSendEmail.json","title":"Script sentry-get-reports"},{"location":"play/tools/#script-sentry-trigger-ebs-scan","text":"The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-ebs-scan help Please specify at least the ec2 instance to be scanned. Usage: INSTANCE = <instance-id> [ REGION = <aws-region> ] [ USERNAME = <username-tag> ] sentry-trigger-ebs-scan Example: INSTANCE = i-0076dab31026905f5 sentry-trigger-ebs-scan If you specify a USERNAME the snapshot(s) will be tagged accordingly. This should ease identifying your own snapshots if using a shared account. Default username is cnctraining . # Trigger scan of EC2 instance i-0076dab31026905f5 existing in the current region INSTANCE = i-0076dab31026905f5 sentry-trigger-ebs-scan Example result: Using region eu-central-1 for user cnctraining State machine is arn:aws:states:eu-central-1:634503960501:stateMachine:ScannerStateMachine-pueSSKvfdN4K Instance volume ( s ) \\n vol-03b25f8105caf9f00 Snapshot snap-0f4cc9d1d8a094861 for volume vol-03b25f8105caf9f00 created { \"ScanID\" : \"634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\" , \"ResourceType\" : \"aws-ebs-volume\" , \"ResourceLocation\" : \"snap-0f4cc9d1d8a094861\" , \"ResourceRegion\" : \"eu-central-1\" , \"MetaData\" : { \"AWSAccountID\" : \"634503960501\" , \"SnapshotID\" : \"snap-0f4cc9d1d8a094861\" , \"VolumeID\" : \"vol-03b25f8105caf9f00\" , \"AttachedInstances\" : [ { \"InstanceID\" : \"i-0076dab31026905f5\" } ] } } { \"executionArn\" : \"arn:aws:states:eu-central-1:634503960501:execution:ScannerStateMachine-pueSSKvfdN4K:Manual-EBS-resource-634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\" , \"startDate\" : \"2023-03-02T14:36:00.763000+00:00\" }","title":"Script sentry-trigger-ebs-scan"},{"location":"play/tools/#script-sentry-trigger-ecr-scan","text":"The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-ecr-scan help Please specify at least the ecr repository to be scanned. Usage: REPO = <repo-name> [ TAG = <image-tag ] [ REGION = <aws-region> ] sentry-trigger-ecr-scan Example: REPO = mawinkler/evil TAG = latest sentry-trigger-ecr-scan # Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region REPO = mawinkler/evil TAG = 0 .1 sentry-trigger-ecr-scan","title":"Script sentry-trigger-ecr-scan"},{"location":"play/tools/#script-sentry-trigger-lambda-scan","text":"The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used. Example calls: # Help sentry-trigger-lambda-scan help Please specify at least the lambda to be scanned. Usage: LAMBDA = <lambda-name> [ REGION = <aws-region> ] sentry-trigger-lambda-scan Example: LAMBDA = cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan # Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region LAMBDA = cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan","title":"Script sentry-trigger-lambda-scan"},{"location":"play/tools/#script-sentry-remove-snapshots","text":"The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the reports. Example calls: # Help sentry-remove-snapshots help Usage: [ REGION = <aws-region> ] [ USERNAME = <username-tag> ] sentry-remove-snapshots Example: USERNAME = cnctraining sentry-remove-snapshots Example result: Using region eu-central-1 for user cnctraining Snapshot ( s ) to delete snap-0f4cc9d1d8a094861 snap-00f0472b795c00644 snap-025d77b5303a37b9d snap-0b357bbc2fef3edad snap-07ee34bf66221579d snap-0b4d1faead597e047 Deleting snapshot snap-0f4cc9d1d8a094861 Deleting snapshot snap-00f0472b795c00644 Deleting snapshot snap-025d77b5303a37b9d Deleting snapshot snap-0b357bbc2fef3edad Deleting snapshot snap-07ee34bf66221579d Deleting snapshot snap-0b4d1faead597e047","title":"Script sentry-remove-snapshots"},{"location":"play/tools/#repo-c1-sentry-reports-to-cloudwatch","text":"Here, I'm describing a simple way to easily get new Sentry reports to CloudWatch using Lambda. Repo Link","title":"Repo C1 Sentry Reports to CloudWatch"},{"location":"play/tools/#cloud-one-container-security","text":"","title":"Cloud One Container Security"},{"location":"play/tools/#scan-image-and-scan-namespace-with-smart-check","text":"The two scripts scan-image and scan-namespace do what you would expect. Running scan-image nginx:latest starts an asynchronous scan of the latest version of nginx. The scan will run on Smart Check, but you are immedeately back in the shell. To access the scan results either use the UI or API of Smart Check. ... { cri t ical : 6 , high : 39 , medium : 40 , low : 13 , ne gligible : 2 , u n k n ow n : 3 } The script scan-namespace.sh scans all used images within the current namespace. Eventually do a kubectl config set-context --current --namespace <NAMESPACE> beforehand to select the namespace to be scanned.","title":"Scan-Image and Scan-Namespace with Smart Check"},{"location":"play/tools/#scan-image-with-artifact-scanning-as-a-service","text":"The script scan-assas do what you would expect, creating a scan request utilizing syft to create an SBOM and upload it to ASaaS for the vulnerability scan. Running scan-assas nginx:latest Should do the trick.","title":"Scan-Image with Artifact Scanning as a Service"},{"location":"play/with-falco/","text":"Play with Falco & Container Security \u00b6 These examples are based on the default Falco ruleset and the additional rules provided by the playground. Networking \u00b6 (PG-NET) Kubernetes Outbound Connection \u00b6 Triggers, if a container is initiating an outbound network communication via TCP or UDP. $ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash root@nginx-6799fc88d8-n5tdd:/# curl www.google.com KShell \u00b6 (PG-KSHELL) Process started in kshell container \u00b6 Triggers, if any process is run in the kshell pod $ kubectl run -it --image = ubuntu kshell --restart = Never --labels = kshell = true --rm -- /bin/bash root@kshell:/# tail /var/log/bootstrap.log (PG-KSHELL) File or directory created in kshell container \u00b6 Triggers, if a file or directory is created in the kshell pod $ kubectl run -it --image = ubuntu kshell --restart = Never --labels = kshell = true --rm -- /bin/bash root@kshell:/# touch foo.txt root@kshell:/# mkdir bar Dangerous Things \u00b6 (PG-IG) Information gathering detected \u00b6 Triggers, if one of the named tools (whoami, nmap, racoon) is run inside a container. $ kubectl run -it busybox --image busybox -- /bin/sh / # whoami (PG-SHELL) Attach/Exec Pod with Terminal User shell in container \u00b6 This rule triggers, if one attaches / executes a shell in a container not running as root. $ cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] securityContext: allowPrivilegeEscalation: false EOF and $ kubectl exec -it security-context-demo -- /bin/sh (PG-SHELL) Attach/Exec Pod with Terminal Root shell in container \u00b6 This rule triggers, if one attaches / executes a shell in a container not running as root. $ kubectl create namespace nginx $ kubectl -n nginx create deployment --image = nginx nginx $ kubectl -n nginx get pods and $ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash (PG-ROOT) Container Run as Root User \u00b6 Rule triggers, if container is started running as root $ kubectl run -it busybox --image busybox -- /bin/sh Integrity Monitoring in Containers \u00b6 (PG-IMC) Detect New File \u00b6 (PG-IMC) Detect New Directory \u00b6 (PG-IMC) Detect File Permission or Ownership Change \u00b6 (PG-IMC) Detect Directory Change \u00b6 Integrity Monitoring on Host and Containers \u00b6 (PG-IM) Kernel Module Modification \u00b6 (PG-IM) Node Created in Filesystem \u00b6 (PG-IM) Listen on New Port \u00b6 Admin Activities \u00b6 (PG-ADM) Detect su or sudo \u00b6 $ sudo su - (PG-ADM) Package Management Launched \u00b6 $ sudo apt update SSH \u00b6 (PG-SSH) Inbound SSH Connection \u00b6 (PG-SSH) Outbound SSH Connection \u00b6 Miscellaneous \u00b6 (PG-KUBECTL) K8s Vulnerable Kubectl Copy \u00b6","title":"With Falco"},{"location":"play/with-falco/#play-with-falco-container-security","text":"These examples are based on the default Falco ruleset and the additional rules provided by the playground.","title":"Play with Falco &amp; Container Security"},{"location":"play/with-falco/#networking","text":"","title":"Networking"},{"location":"play/with-falco/#pg-net-kubernetes-outbound-connection","text":"Triggers, if a container is initiating an outbound network communication via TCP or UDP. $ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash root@nginx-6799fc88d8-n5tdd:/# curl www.google.com","title":"(PG-NET) Kubernetes Outbound Connection"},{"location":"play/with-falco/#kshell","text":"","title":"KShell"},{"location":"play/with-falco/#pg-kshell-process-started-in-kshell-container","text":"Triggers, if any process is run in the kshell pod $ kubectl run -it --image = ubuntu kshell --restart = Never --labels = kshell = true --rm -- /bin/bash root@kshell:/# tail /var/log/bootstrap.log","title":"(PG-KSHELL) Process started in kshell container"},{"location":"play/with-falco/#pg-kshell-file-or-directory-created-in-kshell-container","text":"Triggers, if a file or directory is created in the kshell pod $ kubectl run -it --image = ubuntu kshell --restart = Never --labels = kshell = true --rm -- /bin/bash root@kshell:/# touch foo.txt root@kshell:/# mkdir bar","title":"(PG-KSHELL) File or directory created in kshell container"},{"location":"play/with-falco/#dangerous-things","text":"","title":"Dangerous Things"},{"location":"play/with-falco/#pg-ig-information-gathering-detected","text":"Triggers, if one of the named tools (whoami, nmap, racoon) is run inside a container. $ kubectl run -it busybox --image busybox -- /bin/sh / # whoami","title":"(PG-IG) Information gathering detected"},{"location":"play/with-falco/#pg-shell-attachexec-pod-with-terminal-user-shell-in-container","text":"This rule triggers, if one attaches / executes a shell in a container not running as root. $ cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] securityContext: allowPrivilegeEscalation: false EOF and $ kubectl exec -it security-context-demo -- /bin/sh","title":"(PG-SHELL) Attach/Exec Pod with Terminal User shell in container"},{"location":"play/with-falco/#pg-shell-attachexec-pod-with-terminal-root-shell-in-container","text":"This rule triggers, if one attaches / executes a shell in a container not running as root. $ kubectl create namespace nginx $ kubectl -n nginx create deployment --image = nginx nginx $ kubectl -n nginx get pods and $ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash","title":"(PG-SHELL) Attach/Exec Pod with Terminal Root shell in container"},{"location":"play/with-falco/#pg-root-container-run-as-root-user","text":"Rule triggers, if container is started running as root $ kubectl run -it busybox --image busybox -- /bin/sh","title":"(PG-ROOT) Container Run as Root User"},{"location":"play/with-falco/#integrity-monitoring-in-containers","text":"","title":"Integrity Monitoring in Containers"},{"location":"play/with-falco/#pg-imc-detect-new-file","text":"","title":"(PG-IMC) Detect New File"},{"location":"play/with-falco/#pg-imc-detect-new-directory","text":"","title":"(PG-IMC) Detect New Directory"},{"location":"play/with-falco/#pg-imc-detect-file-permission-or-ownership-change","text":"","title":"(PG-IMC) Detect File Permission or Ownership Change"},{"location":"play/with-falco/#pg-imc-detect-directory-change","text":"","title":"(PG-IMC) Detect Directory Change"},{"location":"play/with-falco/#integrity-monitoring-on-host-and-containers","text":"","title":"Integrity Monitoring on Host and Containers"},{"location":"play/with-falco/#pg-im-kernel-module-modification","text":"","title":"(PG-IM) Kernel Module Modification"},{"location":"play/with-falco/#pg-im-node-created-in-filesystem","text":"","title":"(PG-IM) Node Created in Filesystem"},{"location":"play/with-falco/#pg-im-listen-on-new-port","text":"","title":"(PG-IM) Listen on New Port"},{"location":"play/with-falco/#admin-activities","text":"","title":"Admin Activities"},{"location":"play/with-falco/#pg-adm-detect-su-or-sudo","text":"$ sudo su -","title":"(PG-ADM) Detect su or sudo"},{"location":"play/with-falco/#pg-adm-package-management-launched","text":"$ sudo apt update","title":"(PG-ADM) Package Management Launched"},{"location":"play/with-falco/#ssh","text":"","title":"SSH"},{"location":"play/with-falco/#pg-ssh-inbound-ssh-connection","text":"","title":"(PG-SSH) Inbound SSH Connection"},{"location":"play/with-falco/#pg-ssh-outbound-ssh-connection","text":"","title":"(PG-SSH) Outbound SSH Connection"},{"location":"play/with-falco/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"play/with-falco/#pg-kubectl-k8s-vulnerable-kubectl-copy","text":"","title":"(PG-KUBECTL) K8s Vulnerable Kubectl Copy"}]}