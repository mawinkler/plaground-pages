{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Playground Simplicity","text":"<p>Ultra fast and slim kubernetes playground.</p> <p></p>"},{"location":"#latest-news","title":"Latest News","text":"<p>!!! Playground prepares for Vision One !!!</p> <p>The Playground is preparing for Vision One.</p> <p>Deploy</p> <ul> <li>workload ready to be exploited on an EKS cluster integrated with XDR for Containers</li> <li>virtual machines on AWS integrated with Vision One Endpoint Security (soon)</li> </ul> <p>!!! Playground SIMPLICITY !!!</p> <p>In a nutshell:</p> <ul> <li>Bootstrapping directly from the clouds. It will attempt to upgrade already installed tools to the latest available version.  </li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash &amp;&amp; exit\n</code></pre> <ul> <li>No <code>git clone</code>.</li> <li>No <code>daemon.json</code> configuration.</li> <li>Run the menu by executing <code>playground</code> from anywhere on your system.</li> <li>Bootstrapping has never been easier!</li> </ul>"},{"location":"#change-log","title":"Change Log","text":"<p>07/07/2023</p> <ul> <li>EKS with Amazon Linux nodes now support Application Load Balancing (required for XDR for Containers). The other two variants will follow</li> <li>Built in attack victims are now using ALBs instead of ELB classic</li> </ul>"},{"location":"#currently-work-in-progress","title":"Currently Work in Progress","text":"<ul> <li>Preparing the sub project <code>terraform-awsone</code> to integrate with V1ES for Server &amp; Workload Protection (Windows &amp; Linux)</li> <li>Enable ALB for Bottlerocket and Fargate cluster</li> </ul>"},{"location":"#requirements-and-support-matrix","title":"Requirements and Support Matrix","text":"<p>The Playground is designed to work on these operating systems</p> <ul> <li>Ubuntu Bionic and newer</li> <li>Cloud9 with Ubuntu</li> </ul> <p>for a locally running cluster.</p> <p>The deployment scripts for managed cloud clusters are supporting the following cluster types:</p> <ul> <li>GKE</li> <li>EKS</li> <li>AKS</li> </ul>"},{"location":"#supported-cluster-variants","title":"Supported Cluster Variants","text":"<p>Originally, the playground was designed to create a kubernetes cluster locally on the host running the playground scripts. This is still the fastest way of getting a cluster up and running.</p> <p>In addition to the local cluster, it is also possible to use most functionality of the playground on the managed clusters of the main cloud providers AWS, GCP &amp; Azure as well. Going into this direction requires you to work on a Linux shell and an authenticated CLI to the chosen cloud provider (<code>aws</code>, <code>az</code> or <code>gcloud</code>).</p>"},{"location":"#support-matrix","title":"Support Matrix","text":"Add-On UbuntuLocal Cloud9Local GKECloud EKSCloud AKSCloud Internal Registry X X GCR ECR ACR Scanning Scripts X X X X X C1CS Admission &amp; Continuous X X X X X C1CS Runtime Security X (1) X X X X V1 XDR for Containers X AWS One Playground X Falco X X X X X Gatekeeper X X X X X Open Policy Agent X X X X X Prometheus &amp; Grafana X X X X X Trivy X X X X X Cilium X X X X X Kubescape X X X X X Harbor X (2) Smarthome X (2) Pipelines X Jenkins X X GitLab X X <p>Local means, the cluster will run on the machine you're working on.</p> <p>Cloud means, that the cluster is a cloud managed cluster using the named service.</p> <p>(1) Depending on the Kernel in use. Currently the kernels 4.15.x and 5.4.x are supported.</p> <p>(2) In development.</p>"},{"location":"#cli-commands-of-the-playground","title":"CLI Commands of the Playground","text":"<p>Besides the obvious cli tools like <code>kubectl</code>, <code>docker</code>, etc. the Playground offers you additional commands shown in the table below (and more):</p> Command Function playground The Playground's menu collect-logs-cs Collects logs from Container Security stern Tail logs from multiple pods simultaneouslyExample:<code>stern -n trendmicro-system . -t -s2m</code> syft See github.com/anchore/syft grype See github.com/anchore/grype k9s See k9scli.io"},{"location":"#playgrounds-menu-structure","title":"Playgrounds Menu Structure","text":"<p>The structure of the menu:</p> <pre><code>playground\n\u251c\u2500\u2500 Manage Tools and CSPs...\n\u2502   \u251c\u2500\u2500 Update Tools &amp; Playground\n\u2502   \u251c\u2500\u2500 Install/Update CLI...\n\u2502   \u2502   \u251c\u2500\u2500 AWS CLI\n\u2502   \u2502   \u251c\u2500\u2500 Azure CLI\n\u2502   \u2502   \u2514\u2500\u2500 GCP CLI\n\u2502   \u2514\u2500\u2500 Authenticate to CSP...\n\u2502       \u251c\u2500\u2500 Authenticate to AWS\n\u2502       \u251c\u2500\u2500 Authenticate to Azure\n\u2502       \u2514\u2500\u2500 Authenticate to GCP\n\u251c\u2500\u2500 Manage Clusters...\n\u2502   \u251c\u2500\u2500 Create a Cluster...\n\u2502   \u2502   \u251c\u2500\u2500 Local Cluster\n\u2502   \u2502   \u251c\u2500\u2500 Elastic Kubernetes Cluster\n\u2502   \u2502   \u251c\u2500\u2500 Azure Kubernetes Cluster\n\u2502   \u2502   \u2514\u2500\u2500 Google Kubernetes Engine\n\u2502   \u251c\u2500\u2500 Select Cluster Context...\n\u2502   \u2502   \u2514\u2500\u2500 (Select a Cluster Context)\n\u2502   \u2514\u2500\u2500 (Danger Zone) Tear Down Cluster...\n\u2502       \u251c\u2500\u2500 Local Cluster\n\u2502       \u251c\u2500\u2500 Elastic Kubernetes Cluster\n\u2502       \u251c\u2500\u2500 Azure Kubernetes Cluster\n\u2502       \u2514\u2500\u2500 Google Kubernetes Engine\n\u251c\u2500\u2500 Manage Services...\n\u2502   \u251c\u2500\u2500 Deploy Services...\n\u2502   \u2502   \u2514\u2500\u2500 (Services List)\n\u2502   \u251c\u2500\u2500 (Danger Zone) Delete Services...\n\u2502   \u2502   \u2514\u2500\u2500 (Services List)\n\u2502   \u251c\u2500\u2500 Display Namespaces, LoadBalancers, Deployments &amp; DaemonSets\n\u2502   \u2514\u2500\u2500 Display Services, Addresses and Credentials\n\u2514\u2500\u2500 Manage Configuration...\n    \u251c\u2500\u2500 Display Disk Space\n    \u251c\u2500\u2500 Edit Configuration\n    \u2514\u2500\u2500 Edit daemon.json\n</code></pre>"},{"location":"#good-to-know","title":"Good to Know","text":"<p>If you're curious check out the <code>templates</code>-directory which holds the configuration of all components. Modify at your own risk ;-).</p>"},{"location":"clusters/","title":"Clusters","text":"<p>From within the main menu choose <code>Create Cluster...</code> and select your desired type.</p> <p>Depending on where you have deployed the playground you potentially need to ensure an authenticated cloud CLI.</p> Prerequisites GKE EKS AKS Ubuntu <code>gcloud</code> <code>aws</code> w/ Access Keys <code>az</code> Cloud9 <code>gcloud</code> <code>aws</code> w/ Instance Role (1) <code>az</code> <p>(1) The instance role is automatically created and assigned to the Cloud9 instance during bootstrapping.</p> <p>Then choose your cluster variant to create.</p> <p>If you want to tear down your cluster choose <code>Tear Down Cluster</code> from within the menu. This will destroy the last cluster you created.</p> <p>Note: Cluster versions are defined by the current defaults of the hyper scaler. The built-in cluster is currently version fixed to kubernetes 1.24.7.</p>"},{"location":"deployments/","title":"Deployments","text":"<p>The playground provides a couple of scripts which deploy pre-configured versions of several products. This includes currently:</p> <ul> <li>Container Security</li> <li>Falco Runtime Security</li> <li>Open Policy Agent</li> <li>Gatekeeper</li> <li>Prometheus &amp; Grafana</li> <li>Trivy</li> <li>KUBEClarity</li> <li>Kubescape</li> <li>Harbor</li> <li>Jenkins</li> <li>GitLab</li> <li>Workload Security</li> </ul> <p>Deploy the products via <code>Deploy...</code> in the menu.</p> <p>In addition to the above the playground now supports AWS CodePipelines. The pipeline builds a container image based on a sample repo, scans it with Artifact Scanning as a Service and deploys it with integrated Cloud One Application Security to the EKS cluster.</p> <p>The pipeline requires an EKS. If everything has been set up, running the script <code>deploy-pipeline-aws.sh</code> should do the trick :-). When you're done with the pipeline run the generated script <code>pipeline-aws-down.sh</code> to tear it down.</p>"},{"location":"add-ons/awsone/","title":"Add-On: AWS One Playground","text":"<p>The AWS One Playground is a small environment in AWS and easily created with the help of Terraform.</p> <p>Trend Micro Solutions currently in scope of this environment are:</p> <ul> <li>Vision One</li> <li>Vision One Server &amp; Workload Protection</li> <li>Cloud One Sentry</li> </ul> <p>Three different instances are currently provided by the AWS One Playground with different configurations:</p> <p>Instance Web1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>Nginx and Wordpress deployment</li> <li>Atomic Launcher version 1.0.0.1009</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>Instance Db1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>MySql databse</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>Instance Srv1:</p> <ul> <li>Windows Server 2022</li> <li>Atomic Launcher version 1.0.0.1013</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>Note: V1ES - You need to download the installer packages for Windows and Linux operating systems from your V1ES - Endpoint inventory app. The downloaded files should be named <code>TMServerAgent_Linux_auto_64_Server_-_Workload_Protection_Manager.tar</code> respectively <code>TMServerAgent_Windows_auto_64_Server_-_Workload_Protection_Manager.zip</code> and are to be placed into the directory <code>./terraform-awsone/ec2/files</code> before deploying the environment.</p> <p>Note: Atomic You need to download the Atomic Launcher from here and store it in the <code>./terraform-awsone/files</code>-directory. You do NOT need to unzip the file.</p>"},{"location":"add-ons/awsone/#prepare","title":"Prepare","text":"<p>Ensure the latest AWS CLI via the Playground menu <code>Tools --&gt; CLIs --&gt; AWS</code> and to have authenticated via <code>aws configure</code>.</p> <p>To prepare AWS One Playground demo environmant run</p> <pre><code>deploy-awsone.sh\n</code></pre> <p>and exit the Playground menu. Change in the terraform subdirectory</p> <pre><code>cd $PGPATH/terraform-awsone\n</code></pre>"},{"location":"add-ons/awsone/#optional-adapt-variablestf","title":"Optional: Adapt <code>variables.tf</code>","text":"<p>The <code>variables.tf</code>-file contains the definitions for the AWS region. Adapt it to your needs as required.</p> <p>If you are working on a shared environment you likely want to change <code>private_key_path</code> and <code>public_key_path</code> to make it unique in your environment.</p> <pre><code>variable \"aws_region\" {\ndefault = \"eu-central-1\"\n}\n...\n</code></pre>"},{"location":"add-ons/awsone/#optional-adapt-terraformtfvars","title":"Optional: Adapt <code>terraform.tfvars</code>","text":"<p>The <code>terraform.tfvars</code>-file allows you to restrict internet access to your EC2 instances to only your public IP address. For this you need to change</p> <pre><code>access_ip      = \"0.0.0.0/0\"\n</code></pre> <p>to </p> <pre><code>access_ip      = \"&lt;YOUR IP&gt;/32\"\n</code></pre> <p>The Windows Server get's a local administrator provisioned. Username and password are configured by default to</p> <pre><code>windows_username = \"winadmin1\"\nwindows_password = \"Chang3.M3!\"\n</code></pre>"},{"location":"add-ons/awsone/#deploy-with-terraform","title":"Deploy with Terraform","text":"<p>Prepare your terraform environment by running</p> <pre><code># init modules\nterraform init\n\n# validate configuration\nterraform validate\n</code></pre> <p>Now, you're ready to create your lab environment on AWS :-)</p> <pre><code># plan configuration\nterraform plan -out terraform.out\n\n# apply configuration\nterraform apply terraform.out\n</code></pre> <p>For the impatient, simply run</p> <pre><code>terraform apply --auto-approve\n</code></pre>"},{"location":"add-ons/awsone/#access-the-ec2-instances","title":"Access the EC2 instance(s)","text":"<p>If you want to sniff around on your newly created little instances connect to it via:</p> <pre><code># Linux web1\nssh -i $(terraform output -raw private_key_path) -o StrictHostKeyChecking=no ubuntu@$(terraform output -raw public_instance_ip_web1)\n# Linux db1\nssh -i $(terraform output -raw private_key_path) -o StrictHostKeyChecking=no ubuntu@$(terraform output -raw public_instance_ip_db1)\n</code></pre> <p>To connect to the <code>srv1</code> use Remote Desktop with username/password from <code>terraform.tfvars</code>.</p>"},{"location":"add-ons/awsone/#optional-sserver-workload-protection","title":"Optional: Sserver &amp; Workload Protection","text":"<p>Create Event-Based Tasks to automatically assign Linux or Windows server policies to the machines.</p> <p>Agent-initiated Activation Linux</p> <ul> <li>Actions:</li> <li>Assign Policy: Linux Server</li> <li>Conditions:</li> <li>\"Platform\" matches \".Linux.\"</li> </ul> <p>Agent-initiated Activation Windows</p> <ul> <li>Actions:</li> <li>Assign Policy: Windows Server</li> <li>Conditions:</li> <li>\"Platform\" matches \".Windows.\"</li> </ul>"},{"location":"add-ons/awsone/#create-findings","title":"Create Findings","text":"<p>In the next step we're preparing some findings for Sentry.</p> <pre><code>scripts/create-findings.sh\n</code></pre> <p>Feel free to have a look on the script above, but in theory it should prepare six findings.</p>"},{"location":"add-ons/awsone/#atomicred","title":"AtomicRed","text":"<p>The Windows Server <code>srv1</code> and the Linux Server <code>web1</code> have the Atomic Launcher available. On <code>web1</code> it's located in the home directory of the user, on <code>srv1</code> its within <code>C:\\Windows\\Temp</code>.</p> <p>Unzip password is <code>virus</code>.</p>"},{"location":"add-ons/awsone/#detroy","title":"Detroy","text":"<pre><code># destroy\nterraform destroy\n</code></pre>"},{"location":"add-ons/cilium/","title":"Add-On: Cilium","text":""},{"location":"add-ons/cilium/#install-the-cilium-cli","title":"Install the Cilium CLI","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"add-ons/cilium/#install-cilium","title":"Install Cilium","text":"<pre><code>cilium install\n</code></pre> <p>Validate the Installation</p> <pre><code>cilium status --wait\n</code></pre>"},{"location":"add-ons/cilium/#enable-hubble-in-cilium","title":"Enable Hubble in Cilium","text":"<pre><code>cilium hubble enable\n</code></pre> <p>Validate</p> <pre><code>cilium status\n</code></pre> <pre><code>    /\u00af\u00af\\\n/\u00af\u00af\\__/\u00af\u00af\\    Cilium:         OK\n \\__/\u00af\u00af\\__/    Operator:       OK\n /\u00af\u00af\\__/\u00af\u00af\\    Hubble:         OK\n \\__/\u00af\u00af\\__/    ClusterMesh:    disabled\n    \\__/\n\nDaemonSet         cilium             Desired: 1, Ready: 1/1, Available: 1/1\nDeployment        cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1\nDeployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1\nContainers:       hubble-relay       Running: 1\ncilium             Running: 1\ncilium-operator    Running: 1\nCluster Pods:     3/39 managed by Cilium\nImage versions    cilium             quay.io/cilium/cilium:v1.11.3@sha256:cb6aac121e348abd61a692c435a90a6e2ad3f25baa9915346be7b333de8a767f: 1\ncilium-operator    quay.io/cilium/operator-generic:v1.11.3@sha256:5b81db7a32cb7e2d00bb3cf332277ec2b3be239d9e94a8d979915f4e6648c787: 1\nhubble-relay       quay.io/cilium/hubble-relay:v1.11.3@sha256:7256ec111259a79b4f0e0f80ba4256ea23bd472e1fc3f0865975c2ed113ccb97: 1\n</code></pre> <p>Install the Hubble Client</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\ncurl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check hubble-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\nrm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p>Enable the Hubble UI</p> <pre><code>cilium hubble enable --ui\n</code></pre> <p>Open the Hubble UI</p> <pre><code>cilium hubble ui\n</code></pre>"},{"location":"add-ons/container-security/","title":"Add-On: Container Security","text":""},{"location":"add-ons/container-security/#deploy","title":"Deploy","text":"<p>To deploy Container Security go to <code>Services --&gt; Deploy</code> and choose Container Security.</p>"},{"location":"add-ons/container-security/#access-container-security","title":"Access Container Security","text":"<p>Head over to your Cloud One Account and select the Container Security tile.</p> <p>The deployment script automatically creates a policy for your cluster if it doesn't exist already. Some controls in the deploy section are either set to log or block, the continuous section is set to log only.</p>"},{"location":"add-ons/container-security/#using-a-proxy-with-kind-and-container-security","title":"Using a Proxy with Kind and Container Security","text":""},{"location":"add-ons/container-security/#overrides","title":"Overrides","text":"<p>Add the following section to your overrides of Container Security:</p> <p>Assumes, that the IP of the docker bridge is 172.17.0.1 if using kind. Otherwise simply put the IP/hostname of the proxy instead.</p> <pre><code>proxy:\nhttpProxy: 172.17.0.1:8081\nhttpsProxy: 172.17.0.1:8081\nnoProxy:\n- localhost\n- 127.0.0.1\n- .cluster.local\n</code></pre>"},{"location":"add-ons/container-security/#kind","title":"Kind","text":"<p>Set the following environment variables before creating the cluster</p> <pre><code>export HTTP_PROXY=172.17.0.1:3128\nexport HTTPS_PROXY=172.17.0.1:3128\nexport NO_PROXY=localhost,127.0.0.1\n</code></pre>"},{"location":"add-ons/container-security/#automatically-create-runtime-security-and-sentry-findings","title":"Automatically create Runtime Security and Sentry Findings","text":"<p>To get as much events as possible for runtime detection either uncheck or log only the following parameters in your deployment policy:</p> <ul> <li>Kubectl access</li> <li>attempts to execute in/attach to a container</li> <li>attempts to establish port-forward on a container</li> </ul> <p>Ensure that an exception exists with</p> <p>Allow images with paths that equal <code>mawinkler/atomic_red_docker:latest</code>.</p> <p>Now, run</p> <pre><code>kubectl apply -f $PGPATH/demos/dockerfiles/atomic-red/AtomicRedDocker-FullFalco.yaml\n</code></pre> <p>Soon, you should find quite a lot of events logged for runtime security. Additionally, if Sentry scans your cluster it should detect multiple malwares as well :-).</p>"},{"location":"add-ons/falco/","title":"Add-On: Falco","text":""},{"location":"add-ons/falco/#deploy","title":"Deploy","text":"<p>The deployment of Falco runtime security is very straigt forward with the playground. Simply execute the script <code>deploy-falco.sh</code>, everything else is prepared.</p> <pre><code>deploy-falco.sh\n</code></pre> <p>To ignore events triggerd by services belonging to the playground environment, you can easily whitelist all the playground components by running the following script.</p> <pre><code>whitelist_playground_ns.sh\n</code></pre> <p>Rerun the script whenever you're deploying additional playground components.</p> <p>Falco is integrated with Prometheus and Grafana as well. A Dashboard is available for import with the ID 11914.</p> <p></p>"},{"location":"add-ons/falco/#access","title":"Access","text":"<p>Follow the steps for your platform below. A file called <code>services</code> is either created or updated with the link and the credentials to connect to falco.</p> <p>Linux</p> <p>By default, the Falco UI is on port 8082.</p> <p>Example:</p> <p><code>Falco UI on: http://192.168.1.121:8082/ui/#/</code></p> <p>Cloud9</p> <p>If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browwer. To share Falco over the internet, follow the steps below.</p> <ol> <li>Query the public IP of your Cloud9 instance with</li> </ol> <pre><code>curl http://169.254.169.254/latest/meta-data/public-ipv4\n</code></pre> <ol> <li>In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance</li> <li>Select the security group associated to the instance and select Edit inbound rules.</li> <li>Add an inbound rule for the <code>proxy_listen_port</code> configured in you config.yaml (default: 8082) and choose Source Anywhere</li> <li>Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL.</li> <li>Ensure that an inbound rule is set which allows traffic on the <code>proxy_listen_port</code>. If not, click on <code>Edit inbound rules</code> and add a rule with a low Rule number, Custom TCP, Port range 8443 (or your configured port), Source 0.0.0.0/0 and Allow.</li> </ol> <p>Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s).</p> <p>Examle</p> <p><code>Falco UI: &lt;http://YOUR-CLOUD9-PUBLIC-IP:8082/ui/#/&gt;</code></p>"},{"location":"add-ons/falco/#try-it","title":"Try it","text":"<p>To test the k8s auditing try to create a configmap:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\ndata:\n  ui.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true\n  access.properties: |\n    aws_access_key_id = AKIAXXXXXXXXXXXXXXXX\n    aws_secret_access_key = 1CHPXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nkind: ConfigMap\nmetadata:\n  name: awscfg\nEOF\n</code></pre> <p>If you want to test out own falco rules, create a file called <code>falco/additional_rules.yaml</code> write your rules. It will be included when running <code>deploy-falco.sh</code>.</p> <p>Example:</p> <pre><code>- macro: container\ncondition: container.id != host\n- macro: spawned_process\ncondition: evt.type = execve and evt.dir=&lt;\n- rule: (AR) Run shell in container\ndesc: a shell was spawned by a non-shell program in a container. Container entrypoints are excluded.\ncondition: container and proc.name = bash and spawned_process and proc.pname exists and not proc.pname in (bash, docker)\noutput: \"Shell spawned in a container other than entrypoint (user=%user.name container_id=%container.id container_name=%container.name shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)\"\npriority: WARNING\n</code></pre>"},{"location":"add-ons/falco/#generate-some-events","title":"Generate some events","text":"<pre><code>docker run -it --rm falcosecurity/event-generator run syscall --loop\n</code></pre>"},{"location":"add-ons/falco/#fun-with-privileged-mode","title":"Fun with privileged mode","text":"<pre><code>function shell () {\nkubectl run shell --restart=Never -it --image mawinkler/kod:latest \\\n--rm --attach \\\n--overrides \\\n'\n    {\n      \"spec\":{\n        \"hostPID\": true,\n        \"containers\":[{\n          \"name\":\"kod\",\n          \"image\": \"mawinkler/kod:latest\",\n          \"imagePullPolicy\": \"Always\",\n          \"stdin\": true,\n          \"tty\": true,\n          \"command\":[\"/bin/bash\"],\n          \"nodeSelector\":{\n            \"dedicated\":\"master\"\n          },\n          \"securityContext\":{\n            \"privileged\":true\n          }\n        }]\n      }\n    }\n    '\n}\n</code></pre> <p>You can paste this into a new file <code>shell.sh</code> and source the file.</p> <pre><code>. ./shell.sh\n</code></pre> <p>Then you can type the following to demonstrate a privilege escalation in Kubernetes.</p> <pre><code>shell\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <pre><code>root@shell:/# godmode\nroot@playground-control-plane:/# </code></pre> <p>You're now on the control plane of the cluster and should be kubernetes-admin.</p> <p>If you're wondering what you can do now...</p> <pre><code>mkdir -p /root/.kube\ncp /etc/kubernetes/admin.conf /root/.kube/config\nkubectl auth can-i create deployments -n kube-system\n</code></pre> <pre><code>kubectl create deployment echo --image=inanimate/echo-server\nkubectl get pods\n</code></pre>"},{"location":"add-ons/gatekeeper/","title":"Add-On: Gatekeeper","text":""},{"location":"add-ons/gatekeeper/#deploy","title":"Deploy","text":"<p>Gatekeeper is a customizable admission webhook for Kubernetes that dynamically enforces policies executed by the OPA. Gatekeeper uses CustomResourceDefinitions internally and allows us to define ConstraintTemplates and Constraints to enforce policies on Kubernetes resources such as Pods, Deployments, Jobs.</p> <p>OPA/Gatekeeper uses its own declarative language called Rego, a query language. You define rules in Rego which, if invalid or returned a false expression, will trigger a constraint violation and blocks the ongoing process of creating/updating/deleting the resource.</p> <p>ConstraintTemplate</p> <p>A ConstraintTemplate consists of both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint.</p> <p>Constraint</p> <p>Constraint is an object that says on which resources are the policies applicable, and also what parameters are to be queried and checked to see if they are available in the resource manifest the user is trying to apply in your Kubernetes cluster. Simply put, it is a declaration that its author wants the system to meet a given set of requirements.</p> <p>To deploy run:</p> <pre><code>deploy-gatekeeper.sh\n</code></pre>"},{"location":"add-ons/gatekeeper/#usage","title":"Usage","text":""},{"location":"add-ons/gatekeeper/#example-policy-namespace-label-mandates","title":"Example Policy: Namespace Label Mandates","text":"<p>There is an example within the gatekeeper directory which you can apply by doing</p> <pre><code>kubectl apply -f gatekeeper/constrainttemplate.yaml\nkubectl apply -f gatekeeper/constraints.yaml\n</code></pre> <p>From now on, any new namespace being created requires labels set for <code>stage</code>, <code>status</code> and <code>zone</code>.</p> <p>To test it, run</p> <pre><code>kubectl create namespace nginx --dry-run=true -o yaml | kubectl apply -f -\n</code></pre> <pre><code>Error from server ([label-check] \n\nDENIED. \nReason: Our org policy mandates the following labels: \nYou must provide these labels: {\"stage\", \"status\", \"zone\"}): error when creating \"STDIN\": admission webhook \"validation.gatekeeper.sh\" denied the request: [label-check] \n\nDENIED. \nReason: Our org policy mandates the following labels: \nYou must provide these labels: {\"stage\", \"status\", \"zone\"}\n</code></pre> <p>A valid namespace definition could look like the following:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f - -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: nginx\n  labels:\n    zone: eu-central-1\n    stage: dev\n    status: ready\nEOF\n</code></pre> <p>A subsequent nginx deployment can be done via:</p> <pre><code>kubectl create deployment --image=nginx --namespace nginx nginx\n</code></pre>"},{"location":"add-ons/gitlab/","title":"Add-On: GitLab","text":""},{"location":"add-ons/gitlab/#deploy","title":"Deploy","text":"<p>Note: The script <code>deploy-gitlab.sh</code> deploys a GitLab with Docker Pipeline Support directly on the Docker engine. In other words, GitLab does NOT run on Kubernetes but locally on your machine. This is because GitLab needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind.</p> <p>To deploy GitLab run:</p> <pre><code>deploy-gitlab.sh\n</code></pre> <p>GitLabs configuration is stored on the host within the directories <code>/srv/gitlab</code> and <code>/srv/gitlab-runner</code>. The configuration survives restart and a full delete-deploy operation. If you need to restart from scratch delete the two directories.</p>"},{"location":"add-ons/gitlab/#access-gitlab","title":"Access GitLab","text":"<p>By default GitLab listens on Port 80. The login credentials for the initial login are reported in the <code>Services</code> item of the Playground Menu.</p> <p>http://localhost</p> <p>Cloud9</p> <p>If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share GitLab over the internet, follow the steps below.</p> <ol> <li>Query the public IP of your Cloud9 instance with</li> </ol> <pre><code>curl http://169.254.169.254/latest/meta-data/public-ipv4\n</code></pre> <ol> <li>In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance</li> <li>Select the security group associated to the instance and select Edit inbound rules.</li> <li>Add an inbound rule for the GitLab port (<code>80</code>) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course)</li> <li>Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL.</li> <li>Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on <code>Edit inbound rules</code> and add a rule with a low Rule number, Custom TCP, Port range 80 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow.</li> </ol> <p>You should now be able to connect to GitLab on the public ip of your Cloud9 with your configured port.</p>"},{"location":"add-ons/gitlab/#configure-gitlab","title":"Configure GitLab","text":"<p>Depending on what you're going to do with GitLab some little configuration steps are typically required.</p> <p>Environment Variables</p> <ul> <li>CI_COMMIT_BRANCH: main</li> <li>CI_DEFAULT_BRANCH: main</li> <li>CI_REGISTRY_IMAGE: mawinkler/helloworld</li> <li>CI_REGISTRY_PASSWORD: DOCKER_PASSWORD</li> <li>CI_REGISTRY_USER: mawinkler</li> </ul> <p>Pipeline:</p> <pre><code># Build a Docker image with CI/CD and push to the GitLab registry.\n# Docker-in-Docker documentation: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html\n#\n# This template uses one generic job with conditional builds\n# for the default branch and all other (MR) branches.\ndocker-build:\n# Use the official docker image.\nimage: docker:latest\nstage: build\nservices:\n- docker:dind\nbefore_script:\n- docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\"\n#$CI_REGISTRY\n# Default branch leaves tag empty (= latest tag)\n# All other branches are tagged with the escaped branch name (commit ref slug)\nscript:\n- |\nif [[ \"$CI_COMMIT_BRANCH\" == \"$CI_DEFAULT_BRANCH\" ]]; then\ntag=\"\"\necho \"Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'\"\nelse\ntag=\":$CI_COMMIT_REF_SLUG\"\necho \"Running on branch '$CI_COMMIT_BRANCH': tag = $tag\"\nfi\n- docker build --pull -t \"$CI_REGISTRY_IMAGE${tag}\" .\n- docker push \"$CI_REGISTRY_IMAGE${tag}\"\n# Run this job in a branch where a Dockerfile exists\nrules:\n- if: $CI_COMMIT_BRANCH\nexists:\n- Dockerfile\ntags:\n- docker\n</code></pre> <p>Dockerfile:</p> <pre><code>FROM ubuntu\nCOPY helloworld.sh /\nRUN chmod 700 /helloworld.sh\n\nENTRYPOINT /helloworld.sh\n</code></pre> <p>Script:</p> <pre><code>#!/bin/bash\necho HelloWorld script within Docker image\nexit 0\n</code></pre>"},{"location":"add-ons/harbor/","title":"Add-On: Harbor","text":""},{"location":"add-ons/harbor/#deploy","title":"Deploy","text":"<p>Note: Harbor deployment is currently only supported for local Ubuntu Playgrounds.</p> <p>To deploy Harbor run:</p> <pre><code>deploy-harbor.sh\n</code></pre>"},{"location":"add-ons/harbor/#access","title":"Access","text":"<p>Follow the steps for your platform below. A file called <code>services</code> is either created or updated with the link and the credentials to connect to Harbor.</p> <p>Linux</p> <p>Example:</p> <p><code>Service harbor on: https://192.168.1.121:8085 w/ admin/trendmicro</code></p>"},{"location":"add-ons/istio/","title":"Istio","text":""},{"location":"add-ons/istio/#deploy","title":"Deploy","text":"<pre><code># Add repo\nhelm repo add istio https://istio-release.storage.googleapis.com/charts\nhelm repo update\n\n# Install the Istio base chart\nkubectl create namespace istio-system\nhelm install istio-base istio/base -n istio-system\n\n# Install the Istio discovery chart which deploys the istiod service\nhelm install istiod istio/istiod -n istio-system --wait\n\n# (Optional) Install an ingress gateway\nkubectl create namespace istio-ingress\nkubectl label namespace istio-ingress istio-injection=enabled\nhelm install istio-ingress istio/gateway -n istio-ingress --wait\n\n# Verified using Helm\nhelm status istiod -n istio-system\n</code></pre>"},{"location":"add-ons/jenkins/","title":"Add-On: Jenkins","text":"<p>TODO: Remove Smart Check from sample pipeline</p>"},{"location":"add-ons/jenkins/#deploy","title":"Deploy","text":"<p>Note: The script <code>deploy-jenkins.sh</code> deploys a Jenkins with Docker Pipeline Support and BlueOcean directly on the Docker engine. In other words, Jenkins does NOT run on Kubernetes but locally on your machine. This is because Jenkins needs to have access to the Docker Socket which is typically not available anymore on managed clusters nor easily within Kind.</p> <p>To deploy Jenkins run:</p> <pre><code>deploy-jenkins.sh\n</code></pre>"},{"location":"add-ons/jenkins/#access-jenkins","title":"Access Jenkins","text":"<p>By default Jenkins listens on Port 8080. The login credentials for the initial login are reported in the <code>Services</code> item of the Playground Menu. The password is for one time use only. After logging in to</p> <p>http://localhost:8080</p> <p>You will be prompted for it. In the following you need to create the initial admin user. There are no password policies set, so an <code>admin / trendmicro</code> will work :-).</p> <p>Next, deploy the recommended plug-ins and head over to the Jenkins console.</p> <p>Cloud9</p> <p>If working on a Cloud9 environment you need to adapt the security group of the corresponding EC2 instance to enable access from your browser. To share Jenkins over the internet, follow the steps below.</p> <ol> <li>Query the public IP of your Cloud9 instance with</li> </ol> <pre><code>curl http://169.254.169.254/latest/meta-data/public-ipv4\n</code></pre> <ol> <li>In the IDE for the environment, on the menu bar, choose your user icon, and then choose Manage EC2 Instance</li> <li>Select the security group associated to the instance and select Edit inbound rules.</li> <li>Add an inbound rule for the Jenkins port (<code>8080</code>) configured in you config.yaml and choose Source Anywhere (or your personal IP address of course)</li> <li>Depending on the currently configured Network ACL you might need to add a rule to allow ingoing traffic on the same port. To do this go to the VPC within the Cloud9 instance is running and proceed to the associated Main network ACL.</li> <li>Ensure that an inbound rule is set which allows traffic on the port from above. If not, click on <code>Edit inbound rules</code> and add a rule with a low Rule number, Custom TCP, Port range 8080 (or your configured port), Source 0.0.0.0/0 (or your IP address) and Allow.</li> </ol> <p>You should now be able to connect to Jenkins on the public ip of your Cloud9 with your configured port.</p>"},{"location":"add-ons/jenkins/#configure-jenkins","title":"Configure Jenkins","text":"<p>Depending on what you're going to do with Jenkins some little configuration steps are typically required.</p> <p>I believe the required steps are best demonstrated by an example.</p> <p>Let's assume we want to scan an image from ACR with Smart Check and want to get a pdf report created if the scan reports vulnerability findings. The report is then filed as an artifact within the pipeline run.</p> <p>The pipeline will require two credentials:</p> <ol> <li>Azure Container Registry</li> <li>Smart Check</li> </ol> <p>To create them in Jenkins head over to <code>Manage Jenkins --&gt; Manage Credentials</code>.</p> <p>Create two crendentials in the global domain as <code>Username with password</code>. Name the one for ACR as <code>acr-auth</code> and the smart check one as <code>smartcheck-auth</code>.</p> <p>Additionally, we need to tell Jenkins where to find Smart Check. For this we do create an environment variable. So head over to <code>Manage Jenkins --&gt; Configure System</code>. Scroll down to the <code>Global Properties</code>, check <code>Environment variables</code> and <code>[Add]</code>. Name the variable as <code>DSSC_SERVICE</code> with the IP:Port of your Smart Check instance (e.g. <code>&lt;ip of your server&gt;:8443</code> which is the address of the nginx proxy deployed by the Smart Check deployment script forwarding to your Smart Check running on a kind Kubernetes cluster).</p> <p>Hit <code>[Save]</code>.</p> <p>Then, back on the main dashboard of Jenkins hit <code>+ New Item</code>, enter an item name (e.g. <code>ACR Scan</code>) and click on <code>Pipeline</code> followed by <code>[OK]</code>.</p> <p>For our simple example you can leave all the options unchecked. Into the <code>Pipeline</code>-section paste the following groovy code:</p> <pre><code>pipeline {\nagent any\nstages {\nstage('Test') {\nsteps {\nwithCredentials([\nusernamePassword(\ncredentialsId: 'smartcheck-auth',\nusernameVariable: 'SMARTCHECK_AUTH_CREDS_USR',\npasswordVariable: 'SMARTCHECK_AUTH_CREDS_PSW'\n),\nusernamePassword(\ncredentialsId: 'acr-auth',\nusernameVariable: 'ACR_AUTH_CREDS_USR',\npasswordVariable: 'ACR_AUTH_CREDS_PSW'\n)\n]) { script {\ntry {\nsh \"\"\"\n              docker run deepsecurity/smartcheck-scan-action \\\n                --image-name astrolive.azurecr.io/astrolive:latest \\\n                --smartcheck-host=$DSSC_SERVICE \\\n                --smartcheck-user=$SMARTCHECK_AUTH_CREDS_USR \\\n                --smartcheck-password=$SMARTCHECK_AUTH_CREDS_PSW \\\n                --insecure-skip-tls-verify=true \\\n                --image-pull-auth=\\'{\"username\": \"$ACR_AUTH_CREDS_USR\", \"password\": \"$ACR_AUTH_CREDS_PSW\"}\\'\n              \"\"\"\n} catch(e) {\nscript {\ndocker.image('mawinkler/scan-report').pull()\ndocker.image('mawinkler/scan-report').inside(\"--entrypoint=''\") {\nsh \"\"\"\n                    python /usr/src/app/scan-report.py \\\n                      --config_path \"/usr/src/app\" \\\n                      --name \"astrolive\" \\\n                      --image_tag \"latest\" \\\n                      --out_path \"${WORKSPACE}\" \\\n                      --service \"${DSSC_SERVICE}\" \\\n                      --username \"${SMARTCHECK_AUTH_CREDS_USR}\" \\\n                      --password \"${SMARTCHECK_AUTH_CREDS_PSW}\"\n                  \"\"\"\narchiveArtifacts artifacts: 'report_*.pdf'\n}\nerror('Issues in image found')\n}\n}\n}\n}\n}\n}\n}\n}\n</code></pre> <p>Hit <code>[Save]</code> and start the build with <code>Build Now</code>. If you want see the build progress click on the currently running build in the lower left area of your Jenkins. Within the <code>Console Output</code> you should see the output generated by your first Jenkins pipeline :-).</p>"},{"location":"add-ons/krew/","title":"Add-On: Krew","text":""},{"location":"add-ons/krew/#usage","title":"Usage","text":"<p>Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew. Today, over 130 kubectl plugins are available on Krew.</p> <p>Example usage:</p> <pre><code>kubectl krew list\nkubectl krew install tree\n</code></pre> <p>The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree.</p> <pre><code>kubectl tree node playground-control-plane -A\n</code></pre>"},{"location":"add-ons/kubeclarity/","title":"Add-On: KUBEClarity","text":"<p>KubeClarity is a tool for detection and management of Software Bill Of Materials (SBOM) and vulnerabilities of container images and filesystems. It scans both runtime K8s clusters and CI/CD pipelines for enhanced software supply chain security.</p> <p>Source Repo: here</p>"},{"location":"add-ons/kubeclarity/#deploy-kubeclarity","title":"Deploy KUBEClarity","text":"<p>The deployment of KUBEClarity runtime security is very straigt forward with the playground. Simply execute the script <code>deploy-kubeclarity.sh</code>, everything else is prepared.</p> <pre><code>deploy-kubeclarity.sh\n</code></pre>"},{"location":"add-ons/kubeclarity/#access","title":"Access","text":"<p>Follow the steps for your platform below. A file called <code>services</code> is either created or updated with the link and the credentials to connect to KUBEClarity.</p>"},{"location":"add-ons/kubeclarity/#cli","title":"CLI","text":"<p>KubeClarity includes a CLI that can be run locally and especially useful for CI/CD pipelines. It allows to analyze images and directories to generate SBOM, and scan it for vulnerabilities. The results can be exported to KubeClarity backend.</p>"},{"location":"add-ons/kubeclarity/#installation","title":"Installation","text":"<p>Download the release distribution for your OS from the releases page</p> <p>Unpack the <code>kubeclarity-cli</code> binary, add it to your PATH, and you are good to go!</p> <p>Alternatively, simply run the command below:</p> <pre><code>curl -sL https://github.com/openclarity/kubeclarity/releases/download/v2.14.0/kubeclarity-cli-2.14.0-linux-amd64.tar.gz \\\n-o /tmp/kubeclarity-cli.tar.gz &amp;&amp; \\\ntar xfvz /tmp/kubeclarity-cli.tar.gz kubeclarity-cli &amp;&amp; \\\nmv kubeclarity-cli ${PGPATH}/bin &amp;&amp; \\\nrm /tmp/kubeclarity-cli.tar.gz\n</code></pre>"},{"location":"add-ons/kubeclarity/#sbom-generation","title":"SBOM Generation","text":"<p>Usage: <pre><code>kubeclarity-cli analyze &lt;image/directory name&gt; --input-type &lt;dir|file|image(default)&gt; -o &lt;output file or stdout&gt;\n</code></pre></p> <p>Example: <pre><code>kubeclarity-cli analyze --input-type image nginx:latest -o nginx.sbom\n</code></pre></p> <p>Optionally a list of the content analyzers to use can be configured using the <code>ANALYZER_LIST</code> env variable seperated by a space (e.g <code>ANALYZER_LIST=\"&lt;analyzer 1 name&gt; &lt;analyzer 2 name&gt;\"</code>)</p> <p>Example: <pre><code>ANALYZER_LIST=\"syft gomod\" kubeclarity-cli analyze --input-type image nginx:latest -o nginx.sbom\n</code></pre></p>"},{"location":"add-ons/kubeclarity/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Usage: <pre><code>kubeclarity-cli scan &lt;image/sbom/directoty/file name&gt; --input-type &lt;sbom|dir|file|image(default)&gt; -f &lt;output file&gt;\n</code></pre></p> <p>Example: <pre><code>kubeclarity-cli scan nginx.sbom --input-type sbom\n</code></pre></p> <p>Optionally a list of the vulnerability scanners to use can be configured using the <code>SCANNERS_LIST</code> env variable seperated by a space (e.g <code>SCANNERS_LIST=\"&lt;Scanner1 name&gt; &lt;Scanner2 name&gt;\"</code>)</p> <p>Example: <pre><code>SCANNERS_LIST=\"grype trivy\" kubeclarity-cli scan nginx.sbom --input-type sbom\n</code></pre></p>"},{"location":"add-ons/kubeclarity/#exporting-results-to-kubeclarity-backend","title":"Exporting Results to KubeClarity Backend","text":"<p>To export CLI results to the KubeClarity backend, need to use an application ID as defined by the KubeClarity backend. The application ID can be found in the Applications screen in the UI or using the KubeClarity API.</p>"},{"location":"add-ons/kubescape/","title":"Add-On: Kubescape","text":""},{"location":"add-ons/kubescape/#install-the-kubescape-cli","title":"Install the Kubescape CLI","text":"<pre><code>curl -s https://raw.githubusercontent.com/armosec/kubescape/master/install.sh | /bin/bash\n</code></pre>"},{"location":"add-ons/kubescape/#run-a-full-scan","title":"Run a full Scan","text":"<pre><code>export ARMO_ACCOUNT=XXX\nkubescape scan framework nsa,mitre,armobest \\\n--submit --enable-host-scan --format-version v2 --verbose \\\n--exclude-namespaces=kube-system --fail-threshold 0 \\\n--account ${ARMO_ACCOUNT} --submit\n</code></pre>"},{"location":"add-ons/opa/","title":"Add-On: Open Policy Agent","text":""},{"location":"add-ons/opa/#deploy","title":"Deploy","text":"<p>The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.</p> <p>You don\u2019t have to write policies on your own at the beginning of your journey, OPA and Gatekeeper both have excellent community libraries. You can have a look, fork them, and use them in your organization from here, OPA, and Gatekeeper libraries.</p> <p>To deploy the registry run:</p> <pre><code>deploy-opa.sh\n</code></pre>"},{"location":"add-ons/opa/#usage","title":"Usage","text":""},{"location":"add-ons/opa/#example-policy-registry-whitelisting","title":"Example Policy: Registry Whitelisting","text":"<pre><code>cat &lt;&lt;EOF &gt;opa/registry-whitelist.rego\npackage kubernetes.admission\ndeny[msg] {\n  input.request.kind.kind == \"Pod\"\n  image := input.request.object.spec.containers[_].image\n  not startswith(image, \"172.18.255.1/\")\n  msg := sprintf(\"Image is not from our trusted cluster registry: %v\", [image])\n}\nEOF\nkubectl -n opa create configmap registry-whitelist --from-file=opa/registry-whitelist.rego\n</code></pre> <p>Try to create a deployment</p> <pre><code>kubectl create deployment echo --image=inanimate/echo-server\n</code></pre> <p>If you now run a <code>kubectl get pods</code>, the echo-server should NOT show up.</p> <p>Access the logs from OPA</p> <pre><code>kubectl -n opa logs -l app=opa -c opa -f\n</code></pre> <p>There should be something like</p> <pre><code>\"message\": \"Error creating: admission webhook \\\"validating-webhook.openpolicyagent.org\\\" denied the request: Image is not from our trusted cluster registry: inanimate/echo-server\",\n</code></pre>"},{"location":"add-ons/prometheus-grafana/","title":"Add-On: Prometheus &amp; Grafana","text":""},{"location":"add-ons/prometheus-grafana/#deploy","title":"Deploy","text":"<p>By running <code>deploy-prometheus-grafana.sh</code> you'll get a fully functional and preconfigured Prometheus and Grafana instance on the playground.</p> <p>The following additional scrapers are configured:</p> <ul> <li>api-collector</li> <li>Falco</li> </ul>"},{"location":"add-ons/prometheus-grafana/#access","title":"Access","text":"<p>Follow the steps for your platform below.</p> <p>Linux</p> <p>By default, the Prometheus UI is on port 8081, Grafana on port 8080.</p> <p>Example:</p> <p><code>Prometheus UI on: http://192.168.1.121:8081</code></p> <p><code>Grafana UI on: http://192.168.1.121:8080 w/ admin/trendmicro</code></p> <p>Cloud9</p> <p>See: Access Falco (Cloud9), but use the <code>proxy_listen_port</code>s configured in your config.yaml (default: 8080 (grafana) and 8081 (prometheus)) and choose Source Anywhere. Don't forget to check your inbound rules to allow these ports.</p> <p>Alternatively, you can get the configured port for the service with <code>cat services</code>.</p> <p>Access to the services should then be possible with the public ip of your Cloud9 instance with your configured port(s).</p> <p>Example:</p> <p><code>Grafana: &lt;http://YOUR-CLOUD9-PUBLIC-IP:8080&gt;</code></p> <p><code>Prometheus: &lt;http://YOUR-CLOUD9-PUBLIC-IP:8081&gt;</code></p>"},{"location":"add-ons/registry/","title":"Add-On: Registry","text":""},{"location":"add-ons/registry/#deploy","title":"Deploy","text":"<p>To deploy the registry run:</p> <pre><code>deploy-registry.sh\n</code></pre>"},{"location":"add-ons/registry/#access","title":"Access","text":"<p>Follow the steps for your platform below. A file called <code>services</code> is either created or updated with the link and the credentials to connect to the registry.</p> <p>Linux</p> <p>Example:</p> <p><code>Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin</code></p> <p>Cloud9</p> <p>A file called <code>services</code> is either created or updated with the link and the credentials to connect to the registry.</p> <p>Example:</p> <p><code>Registry login with: echo trendmicro | docker login https://172.18.255.1:5000 --username admin --password-stdin</code></p>"},{"location":"add-ons/trivy/","title":"Add-On: Trivy","text":""},{"location":"add-ons/trivy/#deploy","title":"Deploy","text":"<p>Fundamentally, Trivy gathers security data from various Kubernetes security tools into Kubernetes Custom Resource Definitions (CRD). These extend the Kubernetes APIs so that users can manage and access security reports through the Kubernetes interfaces, like kubectl.</p> <p>To deploy it, run</p> <pre><code>deploy-trivy.sh\n</code></pre>"},{"location":"add-ons/trivy/#usage","title":"Usage","text":"<p>Workload Scanning</p> <pre><code># Vulnerability audit\nkubectl get vulnerabilityreports --all-namespaces -o wide\n\n# Configuration audit\nkubectl get configauditreports --all-namespaces -o wide\n</code></pre> <p>Inspect any of the reports run something like this</p> <pre><code>kubectl describe vulnerabilityreport -n kube-system daemonset-kube-proxy\n</code></pre>"},{"location":"experimenting/deploy-from-private-registry/","title":"Deploy Cloud One Container Security from a Private Registry","text":""},{"location":"experimenting/deploy-from-private-registry/#tools","title":"Tools","text":"<p>Used tools:</p> <ul> <li>Docker</li> <li>yq, awk, helm</li> </ul> <p>Get <code>yq</code></p> <pre><code>curl -L https://github.com/mikefarah/yq/releases/download/v4.24.2/yq_linux_amd64.tar.gz -o yq_linux_amd64.tar.gz\ntar xfvz yq_linux_amd64.tar.gz\nsudo cp yq_linux_amd64 /usr/local/bin/yq\n</code></pre>"},{"location":"experimenting/deploy-from-private-registry/#login-to-the-registries","title":"Login to the Registries","text":"<pre><code>export REGISTRY=172.250.255.1:5000\nexport USERNAME=admin\nexport PASSWORD=trendmicro\n\n# Login to Docker Registry\ndocker login\n\n# Login to private Registry\necho ${PASSWORD} | docker login https://${REGISTRY} --username ${USERNAME} --password-stdin\n</code></pre>"},{"location":"experimenting/deploy-from-private-registry/#container-security-pulltag-push","title":"Container Security - Pull,Tag, &amp; Push","text":"<pre><code># Enumerate the Images\ncurl -L https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz -o master-cs.tar.gz\ntar xfvz master-cs.tar.gz\nexport TAG=$(yq '.images.defaults.tag' cloudone-container-security-helm-master/values.yaml)\necho ${TAG}\n# Pull Container Security images from Dockerhub.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker pull {}\n# Tag the images with your target registry information, making sure to preserve the original image name.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker tag {} ${REGISTRY}/{}\n# Push the images to the private registry\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker push ${REGISTRY}/{}\n# Create image pull secret\nkubectl create secret docker-registry regcred \\\n--docker-server=${REGISTRY} \\\n--docker-username=${USERNAME} \\\n--docker-password=${PASSWORD} \\\n--namespace=container-security\n</code></pre> <p>Update Container Securities <code>overrides.yaml</code> to override the default source registry with your private registry:</p> <pre><code>...\nimages:\ndefaults:\nregistry: [REGISTRY]\ntag: [TAG]\nimagePullSecret: regcred\n</code></pre> <p>Example:</p> <pre><code>...\nimages:\ndefaults:\nregistry: 172.250.255.1:5000\ntag: 2.2.9\nimagePullSecret: regcred\n</code></pre> <p>Deploy Container Security.</p> <pre><code>helm install \\\ncontainer-security \\\n--values $PGPATH/overrides.yaml \\\n--namespace trendmicro-system \\\n--install \\\nhttps://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz\n</code></pre>"},{"location":"experimenting/migrate/","title":"Migrate","text":"<p>This tries to solve the challenge to migrate workload of an existing cluster using public image registries to a trusted, private one (without breaking the services).</p> <p>To try it, being in the playground directory run</p> <pre><code>migrate/save-cluster.sh\n</code></pre> <p>This scripts dumps the full cluster to json files separated by namespace. The namespaces <code>kube-system</code>, <code>registry</code> and <code>default</code> are currently excluded.</p> <p>Effectively, this is a backup of your cluster including ConfigMaps and Secrets etc. which you can deploy on a different cluster easily (<code>kubectl create -f xxx.json</code>)</p> <p>To migrate the images currently in use run</p> <pre><code>migrate/migrate-images.sh\n</code></pre> <p>This second script updates the saved manifests in regards the image location to point them to the private registry. If the image has a digest within it's name it is stripped.</p> <p>The image get's then pulled from the public repo and pushed to the internal one. TODO: This is followed by an image scan and the redeployment.</p> <p>Note: at the time of writing the only supported private registry is the internal one.</p>"},{"location":"experimenting/pipelining-on-aws/","title":"Pipelining on AWS","text":""},{"location":"experimenting/pipelining-on-aws/#requirements-preparations","title":"Requirements &amp; Preparations","text":"<p>Pipelining on AWS requires an EKS cluster, of course. So this pipeline does not work with any other Playground variant.</p>"},{"location":"experimenting/pipelining-on-aws/#deployment","title":"Deployment","text":"<p>Run</p> <pre><code>deploy-pipeline-aws.sh\n</code></pre> <p>This script automates the following:</p> <ol> <li>Creation of a role for CodeBuild - In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster. In this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.</li> <li>Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster. Once the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.</li> <li>We're going to create the AWS CodePipeline using AWS CloudFormation which defines all used resources for our pipeline. In our case this includes ECR, S3, CodeBuild, CodePipeline, CodeCommit, ServiceRoles, and Artifact Scanning as a Service.</li> <li>Next, we're adding a remote repository in AWS CodeCommit which our pipeline will use. At the time of writing the script clones my <code>c1-app-sec-uploader</code>. While doing this we create the kubernetes manifest for our app.</li> <li>Finally, we push our code to the CodeCommit repo which will trigger the pipeline run.</li> </ol> <p>The pipeline builds the container image, pushes it to ECR, scans the image with Artifact Scanning as a Service and finally deploys it to EKS (if the image does not exceed the vulnerability threshold :-))</p> <p>The interaction with Artifact Scanning as a Service is actually done by a containerized <code>tmas</code> client which I'm hosting here: (c1-cs-tmas)[https://github.com/mawinkler/c1-cs-tmas].</p> <p>The deployment obviously can fail if you're running Cloud One Container Security on the cluster, since the image will contain vulnerabilities. So it just depends on you and your defined policy.</p> <p>If everything works you'll have a running uploader demo on your cluster.</p>"},{"location":"experimenting/pipelining-on-aws/#further-reading","title":"Further Reading","text":"<ul> <li>AWS EKS Cluster Authentication: https://docs.aws.amazon.com/eks/latest/userguide/cluster-auth.html</li> </ul>"},{"location":"experimenting/pipelining-on-aws/#tear-down","title":"Tear Down","text":"<p>To tear down the pipeline including CodeCommit, Roles etc. simply run the auto-generated script</p> <pre><code>pipeline-aws-down.sh\n</code></pre>"},{"location":"getting-started/cluster-variants/","title":"Cluster Variants","text":"<p>The Playground has support for the following Kubernetes cluster variants. They all come preconfigured and ready to be used:</p> <ul> <li>Local Cluster (Kind)</li> <li>AWS EKS with Amazon Linux nodes</li> <li>AWS EKS with Bottlerocket nodes (in Private Preview)</li> <li>AWS EKS with Bottlerocket nodes and Fargate profile (in Private Preview)</li> <li>Azure Kubernetes Cluster</li> <li>Google Kubernetes Engine</li> </ul> <p>When to use which variant: Besides the obvious reason that you want to test or learn something specific you are pretty much flexible. Most of the provided services run on each cluster type. Personally, when I just need to test something quickly I simply choose the local cluster because it is up in 2 minutes, provides the full Kubernetes functionality and is destroyed in 2 seconds. Important to note: All managed clusters by the CSPs create costs! Since the playground configures everything for you you should tear down these clusters when you're done. This means, and this is the approach I'm using, when I want to have an always there cluster, I run the local cluster on a local machine in my own lab.</p>"},{"location":"getting-started/cluster-variants/#local-cluster-kind","title":"Local Cluster (Kind)","text":"<p>This variant is actually the fastest and cheapest variant of Kubernetes clusters available in the Playground. It uses the greate open source project <code>kind</code>. kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. kind was primarily designed for testing Kubernetes itself, but may be used for local development, learning and testing things.</p> <p>You can very quickly create this kind of clusters on any Ubuntu server, whereby I'm testing it currently on a Ubuntu 20.04 LTS and a Cloud9 with an Ubuntu operating system. Other platforms might work, but are untested.</p> <p>Some specs:</p> <ul> <li>Performance depends on the host, of course</li> <li>MetalLB as the Load Balancer</li> <li>Ingress Controller based on Nginx</li> <li>Calico Pod Network</li> <li>Automatic proxy configuration on host level</li> </ul>"},{"location":"getting-started/cluster-variants/#aws-eks-with-amazon-linux-nodes","title":"AWS EKS with Amazon Linux nodes","text":"<p>This is the current configuration for a standard EKS cluster:</p> <pre><code>managedNodeGroups:\n- name: nodegroup\ninstanceType: m5.large\nminSize: 2\nmaxSize: 4\ndesiredCapacity: 2\niam:\nwithAddonPolicies:\nalbIngress: true\nebs: true\ncloudWatch: true\nautoScaler: true\nawsLoadBalancerController: true\n</code></pre> <p>Some additional specs:</p> <ul> <li>Nodes EC2 type currently m5.large</li> <li>Secrets encryption is enabled</li> <li>CloudWatch logging is enabled</li> <li>Calico (Tigera) pod network</li> <li>Amazon EBS CSI driver is deployed</li> </ul>"},{"location":"getting-started/cluster-variants/#aws-eks-with-bottlerocket-nodes","title":"AWS EKS with Bottlerocket nodes","text":"<p>This is the current configuration for a standard EKS cluster with Bottlerocket nodes:</p> <pre><code>managedNodeGroups:\n- name: nodegroup\ninstanceType: m5.large\nminSize: 2\nmaxSize: 4\ndesiredCapacity: 2\namiFamily: Bottlerocket\niam:\nwithAddonPolicies:\nalbIngress: true\nebs: true\ncloudWatch: true\nautoScaler: true\nawsLoadBalancerController: true\ntags:\nnodegroup-type: Bottlerocket\n</code></pre> <p>Some additional specs:</p> <ul> <li>Secrets encryption is enabled</li> <li>CloudWatch logging is enabled</li> <li>Calico (Tigera) pod network</li> <li>Amazon EBS CSI driver is deployed</li> </ul>"},{"location":"getting-started/cluster-variants/#aws-eks-with-bottlerocket-nodes-and-fargate-profile","title":"AWS EKS with Bottlerocket nodes and Fargate profile","text":"<p>This is the current configuration for a standard EKS cluster with Bottlerocket nodes and an additional Fargate profile:</p> <pre><code>managedNodeGroups:\n- name: nodegroup\ninstanceType: m5.large\nminSize: 2\nmaxSize: 4\ndesiredCapacity: 4\namiFamily: Bottlerocket\niam:\nwithAddonPolicies:\nalbIngress: true\nebs: true\ncloudWatch: true\nautoScaler: true\nawsLoadBalancerController: true\ntags:\nnodegroup-type: Bottlerocket\nfargateProfiles:\n- name: fp-default\nselectors:\n# All workloads in the \"default\" and \"kube-system\" Kubernetes\n# namespace will be scheduled onto Fargate:\n- namespace: default\n- namespace: kube-system\n- namespace: victims\n</code></pre> <p>The cluster will behave as normally, but the workload within the namespaces <code>default</code>, <code>kube-system</code>, and <code>victims</code> will actually run on Fargate.</p>"},{"location":"getting-started/cluster-variants/#azure-kubernetes-cluster","title":"Azure Kubernetes Cluster","text":"<p>Here, you'll get an AKS cluster on Azure.</p> <p>Some additional specs:</p> <ul> <li>Two nodes</li> <li>Monitoring is enabled</li> </ul>"},{"location":"getting-started/cluster-variants/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<ul> <li>Nodes e2-standard-4 (4CPU/16GB per node)</li> </ul>"},{"location":"getting-started/configuration-simplicity/","title":"Getting Started Configuration","text":"<p>From within the main menu choose <code>Edit Configuration</code></p> <p>Typically you don't need to change much here besides setting your api-key and region for Cloud One.</p> <p>If you intent to use Artifact Scanning as a Service create a scanner api-key in Cloud One and set it as <code>scanner_api_key</code>.</p> <p>When willing to play with the built-in AWSONE environment (<code>terraform-awsone</code>) you need to set the relevant values for Workload Security and Vision One, of course.</p> <pre><code>## Kubernetes cluster name\n##\n## Default value: playground\ncluster_name: playground\n## Kubernetes cluster node instance type (AWS)\n##\n## Default value: playground\ncluster_instance_type: t3.medium\n## Editor for Playground. Defaults to autodetection of nano over vim and vi\n##\n## Default value: ''\neditor: vim\nservices:\n- name: cloudone\n## Cloud One region to work with\n## \n## Default value: trend-us-1\nregion: us-1\n## Cloud One instance to use\n##\n## Allowed values: cloudone, staging-cloudone, dev-cloudone\n## \n## Default value: cloudone\ninstance: cloudone\n## Cloud One API Key with Full Access\n## \n## REQUIRED if you want to play with Cloud One\n##\n## Default value: ''\napi_key: YOUR CLOUD ONE API KEY HERE\n## Cloud One Scanner API Key\n## \n## REQUIRED if you want to play with Artifac Scanning as a Service\n##\n## Default value: ''\nscanner_api_key: ''\n## Cloud One Workload Security Tenant ID\n## \n## REQUIRED if you want to play with Cloud One Workload Security\n##\n## Default value: ''\nws_tenant_id: ''\n## Cloud One Workload Security Token\n## \n## REQUIRED if you want to play with Cloud One Workload Security\n##\n## Default value: ''\nws_token: ''\n## Cloud One Workload Security Linux Policy ID\n## \n## REQUIRED if you want to play with Cloud One Workload Security\n##\n## Default value: ''\nws_policy_id: ''\n- name: visionone\n## Vision One Basecamp agent download url\n##\n## REQUIRED if you want to play with Vision One\n##\n## Default value: ''\nxbc_agent_url: ''\n- name: container_security\n## The name of the created or reused policy\n## \n## Default value: relaxed_playground\npolicy_name: relaxed_playground\n## Target namespace for Smart Check\n## \n## Default value: trendmicro-system\nnamespace: trendmicro-system\n# ================ DO NOT CHANGE ANYTHING BELOW THIS LINE ===============\n# ================== UNLESS YOU KNOW WHAT YOU'RE DOING ==================\n...\n</code></pre>"},{"location":"getting-started/kind-simplicity/","title":"Getting Started w/ the built in Cluster","text":"<p>Choose the platform documentation</p>"},{"location":"getting-started/kind-simplicity/#ubuntu","title":"Ubuntu","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a Ubuntu machine (not Cloud9) and</li> <li>are going to use the built in cluster</li> </ul> <p>Test if <code>sudo</code> requires a password by running <code>sudo ls /etc</code>. If you don't get a password prompt you're fine, otherwise run.</p> <pre><code>sudo visudo -f /etc/sudoers.d/custom-users\n</code></pre> <p>Add the following line:</p> <pre><code>&lt;YOUR USER NAME&gt; ALL=(ALL) NOPASSWD:ALL </code></pre> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash &amp;&amp; exit\n</code></pre> <p>Choose Bootstrap.</p> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue.</p> <p>To restart the menu execute</p> <pre><code>playground\n</code></pre> <p>from anywhere in your terminal.</p> <p>Finally, create your local cluster by choosing <code>Create Cluster...</code> --&gt; <code>Local Cluster</code>.</p>"},{"location":"getting-started/kind-simplicity/#cloud9","title":"Cloud9","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a AWS Cloud9 environment and</li> <li>are going to use the built in cluster</li> </ul> <p>Follow the steps below to create a Cloud9 suitable for the Playground.</p> <ul> <li>Point your browser to AWS</li> <li>Choose your default AWS region in the top right</li> <li>Go to the Cloud9 service</li> <li>Select <code>[Create Cloud9 environment]</code></li> <li>Name it as you like</li> <li>Choose <code>[t3.xlarge]</code> for instance type and</li> <li><code>Ubuntu 18.04 LTS</code> as the platform</li> <li>For the rest take all default values and click <code>[Create environment]</code></li> </ul> <p>Update IAM Settings for the Workspace</p> <ul> <li>Click the gear icon (in top right corner), or click to open a new tab and choose <code>[Open Preferences]</code></li> <li>Select AWS SETTINGS</li> <li>Turn OFF <code>[AWS managed temporary credentials]</code></li> <li>Close the Preferences tab</li> </ul> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash &amp;&amp; exit\n</code></pre> <p>If you run the above command on a newly created or rebooted Cloud9 instance and are receiving the following error, just wait a minute or two and rerun the curl command. The reason for this error is, that directly after starting the machine some update processes are running in the background causing the lock to the package manager process.</p> <pre><code>E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?\n</code></pre> <p>Choose <code>Bootstrap</code>. You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance.</p> <p>If you forgot to disable AWS managed temporary credentials you will asked to do it again.</p> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue.</p> <p>To restart the menu execute</p> <pre><code>playground\n</code></pre> <p>from anywhere in your terminal.</p> <p>Finally, create your local cluster by choosing <code>Create Cluster...</code> --&gt; <code>Local Cluster</code>.</p>"},{"location":"getting-started/managed-simplicity/","title":"Getting Started w/ managed Clusters","text":"<p>Choose the platform documentation</p>"},{"location":"getting-started/managed-simplicity/#ubuntu","title":"Ubuntu","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a Ubuntu machine and</li> <li>are going to use either EKS, AKS or GKE</li> </ul> <p>Test if <code>sudo</code> requires a password by running <code>sudo ls /etc</code>. If you don't get a password prompt you're fine, otherwise run.</p> <pre><code>sudo visudo -f /etc/sudoers.d/custom-users\n</code></pre> <p>Add the following line:</p> <pre><code>&lt;YOUR USER NAME&gt; ALL=(ALL) NOPASSWD:ALL </code></pre> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash &amp;&amp; exit\n</code></pre> <p>Choose Bootstrap.</p> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue.</p> <p>To restart the menu execute</p> <pre><code>playground\n</code></pre> <p>from anywhere in your terminal.</p> <p>Now, choose the option <code>Install/Update CLI...</code> --&gt; <code>AWS CLI</code>, <code>Azure CLI</code>, or <code>GCP CLI</code>.</p> <p>Next, you need to ensure that your CLI is authenticated. Do this via the option <code>Authenticate to CSP...</code> and follow the instructions.</p> <p>Finally, create your cluster by choosing <code>Create Cluster...</code> --&gt; <code>EKS</code>, <code>AKS</code>, or <code>GKE</code>.</p>"},{"location":"getting-started/managed-simplicity/#cloud9","title":"Cloud9","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a AWS Cloud9 environment and</li> <li>are going to use EKS as the cluster</li> </ul> <p>Follow the steps below to create a Cloud9 suitable for the Playground.</p> <ul> <li>Point your browser to AWS</li> <li>Choose your default AWS region in the top right</li> <li>Go to the Cloud9 service</li> <li>Select <code>[Create Cloud9 environment]</code></li> <li>Name it as you like</li> <li>Choose <code>[t3.medium]</code> for instance type and</li> <li><code>Ubuntu 18.04 LTS</code> as the platform</li> <li>For the rest take all default values and click <code>[Create environment]</code></li> </ul> <p>Update IAM Settings for the Workspace</p> <ul> <li>Click the gear icon (in top right corner), or click to open a new tab and choose <code>[Open Preferences]</code></li> <li>Select AWS SETTINGS</li> <li>Turn OFF <code>[AWS managed temporary credentials]</code></li> <li>Close the Preferences tab</li> </ul> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/c1-playground/master/bin/playground | bash &amp;&amp; exit\n</code></pre> <p>If you run the above command on a newly created or rebooted Cloud9 instance and are receiving the following error, just wait a minute or two and rerun the curl command. The reason for this error is, that directly after starting the machine some update processes are running in the background causing the lock to the package manager process.</p> <pre><code>E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?\n</code></pre> <p>Choose <code>Bootstrap</code>. You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance.</p> <p>If you forgot to disable AWS managed temporary credentials you will asked to do it again.</p> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue.</p> <p>To restart the menu execute</p> <pre><code>playground\n</code></pre> <p>from anywhere in your terminal.</p> <p>Now, choose the option <code>Install/Update CLI...</code> --&gt; <code>AWS CLI</code>, <code>Azure CLI</code>, or <code>GCP CLI</code>.</p> <p>If you're going to use EKS, you don't need to do anything additionally before creating the EKS cluster.</p> <p>In the case of AKS or GKE, you need to authenticate to Azure or GCP first. To authenticate to your CSP choose the option <code>Authenticate to CSP...</code> and follow the instructions.</p> <p>Finally, create your cluster by choosing <code>Create Cluster...</code> --&gt; <code>EKS</code>, <code>AKS</code>, or <code>GKE</code>.</p>"},{"location":"play/scripts/","title":"Demo Scripts","text":"<p>Note: Demo scripts are now located in the repo Playground Demos</p> <p>TODO: Migrate to ASaaS</p> <p>The Playground supports automated scripts to demonstrate functionalities of deployments. Currently, there are two scripts available showing some capabilities of Cloud One Container Security.</p> <p>To run them, ensure to have an EKS cluster up and running and have Smart Check and Container Security deployed.</p> <p>After configuring the policy and rule set as shown below, you can run the demos with</p> <pre><code># Deployment Control Demo\n./demos/demo-c1cs-dc.sh\n\n# Runtime Security Demo\n./demos/demo-c1cs-rt.sh\n</code></pre>"},{"location":"play/scripts/#deployment-control-demo","title":"Deployment Control Demo","text":"<p>Storyline: A developer wants to try out a new <code>nginx</code> image but fails since the image has critical vulnerabilities, he tries to deploy from docker hub etc. Lastly he tries to attach to the pod, which is prevented by Container Security.</p> <p>To prepare for the demo verify that the cluster policy is set as shown below:</p> <ul> <li>Pod properties</li> <li>uncheck - containers that run as root</li> <li>Block - containers that run in the host network namespace</li> <li>Block - containers that run in the host IPC namespace</li> <li>Block - containers that run in the host PID namespace</li> <li>Container properties</li> <li>Block - containers that are permitted to run as root</li> <li>Block - privileged containers</li> <li>Block - containers with privilege escalation rights</li> <li>Block - containers that can write to the root filesystem</li> <li>Image properties</li> <li>Block - images from registries with names that DO NOT EQUAL REGISTRY:PORT</li> <li>uncheck - images with names that</li> <li>Log - images with tags that EQUAL latest</li> <li>uncheck - images with image paths that</li> <li>Scan Results</li> <li>Block - images that are not scanned</li> <li>Block - images with malware</li> <li>Log - images with content findings whose severity is CRITICAL OR HIGHER</li> <li>Log - images with checklists whose severity is CRITICAL OR HIGHER</li> <li>Log - images with vulnerabilities whose severity is CRITICAL OR HIGHER</li> <li>Block - images with vulnerabilities whose CVSS attack vector is NETWORK and whose severity is HIGH OR HIGHER</li> <li>Block - images with vulnerabilities whose CVSS attack complexity is LOW and whose severity is HIGH OR HIGHER</li> <li>Block - images with vulnerabilities whose CVSS availability impact is HIGH and whose severity is HIGH OR HIGHER</li> <li>Log - images with a negative PCI-DSS checklist result with severity CRITICAL OR HIGHER</li> <li>Kubectl Access</li> <li>Block - attempts to execute in/attach to a container</li> <li>Log - attempts to establish port-forward on a container</li> </ul> <p>Most of it should already configured by the <code>deploy-container-security.sh</code> script.</p> <p>Run the demo being in the playground directory with</p> <pre><code>./demos/demo-c1cs-dc.sh\n</code></pre>"},{"location":"play/scripts/#runtime-security-demo","title":"Runtime Security Demo","text":"<p>Storyline: A kubernetes admin newbie executes some information gathering about the kubernetes cluster from within a running pod. Finally, he gets kicked by Container Security because of the <code>kubectl</code> usage.</p> <p>To successfully run the runtime demo you need adjust the aboves policy slightly.</p> <p>Change:</p> <ul> <li>Kubectl Access</li> <li> <p>Log - attempts to execute in/attach to a container</p> </li> <li> <p>Exceptions</p> </li> <li>Allow images with paths that equal <code>docker.io/mawinkler/ubuntu:latest</code></li> </ul> <p>Additionally, set the runtime rule <code>(T1543)Launch Package Management Process in Container</code> to Log. Normally you'll find that rule in the <code>*_error</code> ruleset.</p> <p>Run the demo being in the playground directory with</p> <pre><code>./demos/demo-c1cs-rt.sh\n</code></pre> <p>The demo starts locally on your system, but creates a pod in the <code>default</code> namespace of your cluster using a slightly pimped ubuntu image which is pulled from my docker hub account. The main demo runs within that pod on the cluster, not on your local machine.</p> <p>The Dockerfile for this image is in <code>./demos/pod/Dockerfile</code> for you to verify, but you do not need to build it yourself.</p>"},{"location":"play/tools/","title":"Tools &amp; Scripts","text":""},{"location":"play/tools/#disclaimer","title":"Disclaimer","text":"<p>All the scripts and tools described here are at a proof-of-concept level. Some finetuning for production use is advised. Additionally, they might not be officially supported by the Cloud One solutions :-).</p>"},{"location":"play/tools/#cloud-one-sentry","title":"Cloud One Sentry","text":"<p>Within the <code>tools</code> directory are some scripts for Sentry:</p> <ol> <li><code>sentry-get-reports</code> - Downloads all Sentry reports generated within the last 24hs to your local directory</li> <li><code>sentry-get-logs</code> - Downloads all Sentry logs generated within the last 24hs to your local directory</li> <li><code>sentry-get-cloudwatch-logs</code> - Downloads Sentry CloudWatch logs for AM, IM, Parse Volume and Reports</li> <li><code>sentry-get-last-failed-executions</code> - Returns metadata of recently failed state machine runs</li> <li><code>sentry-trigger-ebs-scan</code> - Trigger a full scan for a given EC2 instance with one or more EBS volumes attached</li> <li><code>sentry-trigger-ecr-scan</code> - Trigger a full scan for a given ECR repo</li> <li><code>sentry-trigger-ebs-scan</code> - Trigger a full scan for a given Lambda</li> <li><code>sentry-remove-snapshots</code> - Delete snapshots created by <code>sentry-trigger-ebs-scan</code></li> </ol>"},{"location":"play/tools/#script-sentry-get-reports","title":"Script <code>sentry-get-reports</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the reports.</p> <p>Example calls:</p> <pre><code># Help\nsentry-get-reports help\n</code></pre> <pre><code>Usage: [RELATIVETIME=&lt;relative time in minutes&gt;] [REGION=&lt;aws-region&gt;] sentry-get-reports\n\nExample:\n  RELATIVETIME=30 REGION=eu-central-1 sentry-get-reports\n</code></pre> <pre><code># Get the reports from the current region\nsentry-get-reports\n\n# Get the reports from a region other than your current region\nREGION=us-east-1 sentry-get-reports\n</code></pre> <p>Example result:</p> <pre><code>sentry-reports-2023-02-13_15-42-54\n\u251c\u2500\u2500 aws-ebs-volumes\n\u2502   \u251c\u2500\u2500 i-0076dab31026905f5-vol-0c84097ebd8024cc0.json\n\u2502   \u251c\u2500\u2500 i-03c03a975195bf87b-vol-0fc32faf32aaf378e.json\n\u2502   \u2514\u2500\u2500 i-08a345a13318f7db0-vol-036b3cf8196cea1c0.json\n\u251c\u2500\u2500 aws-ecr-images\n\u2502   \u251c\u2500\u2500 634503960501.dkr.ecr.eu-central-1.amazonaws.com_busybox:latest.json\n\u2502   \u251c\u2500\u2500 634503960501.dkr.ecr.eu-central-1.amazonaws.com_hello-world:latest.json\n\u2502   \u251c\u2500\u2500 634503960501.dkr.ecr.eu-central-1.amazonaws.com_mawinkler_evil:latest.json\n\u2502   \u251c\u2500\u2500 634503960501.dkr.ecr.eu-central-1.amazonaws.com_nginx:1.21.6.json\n\u2502   \u2514\u2500\u2500 634503960501.dkr.ecr.eu-central-1.amazonaws.com_serverless-webhook:latest.json\n\u2514\u2500\u2500 aws-lambda-functions\n    \u251c\u2500\u2500 CloudOneWorkloadSecurityUS1SNSPublish.json\n    \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-CreateLambdaAliasLambda-UPdmsJoha1xM.json\n    \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-GetLambdaLastConfig-Uj2V8nohPYhb.json\n    \u251c\u2500\u2500 Scanner-TM-FileStorageSecurity-ScannerLambda-mEoyirF86l1J.json\n    \u251c\u2500\u2500 Scanner-TM-FileStorageSecu-ScannerDeadLetterLambda-ftXbhuqoFN2b.json\n    \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-gq0psizTvl7R.json\n    \u251c\u2500\u2500 ScheduledScan-TM-FileStorageSecurit-BucketFullScan-MyKEV22XwkY6.json\n    \u251c\u2500\u2500 SecurityHubStack-CreateIntegrationFunctionB363DF0B-Jz2BcKzj5NvD.json\n    \u251c\u2500\u2500 serverless-webhook.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanAM-LC9XncBvNApb.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a354-4-sideScanIM-xImAvOBHiHRx.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28bbb-a3-sideScanReport-t3DwsQNXYVoj.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-ScanInvokerFunction-NnmdRDpmPbH1.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a28b-sideScanParseVolume-8Ya9840jWIuv.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a2-LambdaUpdaterFunction-HZpcwfJXwNTb.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8a-SendScanReportFunction-2X3KHz69ulrV.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f8-ScanSQSConsumerFunction-Y24t3VpgJ4eG.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f-deleteEbsSnapshotFunctio-YsSJWo4pwcOP.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f-ecrResourceProviderFunct-sL0O6KdHpsWK.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f-lambdaResourceProviderFu-VdfrMST0mtXQ.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f-snapshotDistributionFunc-s1faGUTYT8sd.json\n    \u251c\u2500\u2500 StackSet-SentryStackSet-f-SnapshotProviderFunction-07HwDBE41qO0.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-IMPZcIjLGFTj.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSec-SetupBucketNotificationL-R8mg71T8Qj7V.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-bSaQH7juC9ZV.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecu-CreateLambdaAliasLambda-zkG5ey8gSFje.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-5IMogaWAeCBy.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecu-PostScanActionTagLambda-B5cdL2YrPbk0.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-a2bmnXZRLzxw.json\n    \u251c\u2500\u2500 Storage-TM-FileStorageSecurit-BucketListenerLambda-C48dmTDLXKBK.json\n    \u2514\u2500\u2500 Storage-TM-FileStorageSecurity_ScanSendEmail.json\n</code></pre>"},{"location":"play/tools/#script-sentry-get-logs","title":"Script <code>sentry-get-logs</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used to query the logs.</p> <p>Example calls:</p> <pre><code># Help\nsentry-get-logs help\n</code></pre> <pre><code>Usage: [RELATIVETIME=&lt;relative time in minutes&gt;] [REGION=&lt;aws-region&gt;] sentry-get-logs\n\nExample:\n  RELATIVETIME=30 REGION=eu-central-1 sentry-get-logs\n</code></pre> <pre><code># Get the logs from the current region\nsentry-get-logs\n\n# Get the logs from a region other than your current region\nREGION=us-east-1 sentry-get-logs\n</code></pre> <p>Example result:</p> <pre><code>sentry-logs-2023-03-06_14-47-57\n\u2514\u2500\u2500 sentry-sentry-logs-2023-03-06_14-47-57.log\n</code></pre>"},{"location":"play/tools/#script-sentry-get-cloudwatch-logs","title":"Script <code>sentry-get-cloudwatch-logs</code>","text":"<p>Example calls:</p> <pre><code># Help\nsentry-get-cloudwatch-logs help\n</code></pre> <pre><code>Usage: [RELATIVETIME=&lt;relative time in minutes&gt;] [REGION=&lt;aws-region&gt;] sentry-get-cloudwatch-logs\n\nExample:\n  RELATIVETIME=30 REGION=eu-central-1 sentry-get-cloudwatch-logs\n</code></pre> <pre><code># Get the logs from the past hour from the current region\nsentry-get-cloudwatch-logs\n\n# Get the logs from the past 30 minutes from a region other than your current region\nRELATIVETIME=30 REGION=us-east-1 sentry-get-cloudwatch-logs\n</code></pre> <p>Example result:</p> <pre><code>sentry-cloudwatch-logs-2023-03-07_10-18-57\n\u251c\u2500\u2500 sideScanAM.log\n\u251c\u2500\u2500 sideScanIM.log\n\u251c\u2500\u2500 sideScanPV.log\n\u2514\u2500\u2500 sideScanRE.log\n</code></pre>"},{"location":"play/tools/#script-sentry-get-last-failed-executions","title":"Script <code>sentry-get-last-failed-executions</code>","text":"<p>Example calls:</p> <pre><code># Help\nsentry-get-last-failed-executions help\n</code></pre> <pre><code>Usage: [RELATIVETIME=&lt;relative time in minutes&gt;] [REGION=&lt;aws-region&gt;] sentry-get-last-failed-executions\n\nExample:\n  RELATIVETIME=60 sentry-get-last-failed-executions\n</code></pre> <pre><code># Get the recently failed executions\nsentry-get-last-failed-executions\n\n# Get the failed executions from the past 30 minutes from a region other than your current region\nRELATIVETIME=30m REGION=us-east-1 sentry-get-last-failed-executions\n</code></pre> <p>Example result:</p> <pre><code>Using region eu-central-1\nRelative time 30m\nState machine is arn:aws:states:eu-central-1:634503960501:stateMachine:ScannerStateMachine-YPTDbTnpolcR\nFailed executions: 1\n{\n\"AWSAccountID\": \"634503960501\",\n  \"SnapshotID\": \"snap-0e0c0376ae6271d9d\",\n  \"VolumeID\": \"vol-01e7d57f91114b4c3\",\n  \"AttachedInstances\": [\n{\n\"InstanceID\": \"i-06342402d2d170aff\"\n}\n]\n}\n</code></pre>"},{"location":"play/tools/#script-sentry-trigger-ebs-scan","title":"Script <code>sentry-trigger-ebs-scan</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used.</p> <p>Example calls:</p> <pre><code># Help\nsentry-trigger-ebs-scan help\n</code></pre> <pre><code>Please specify at least the ec2 instance to be scanned.\n\nUsage: INSTANCE=&lt;instance-id&gt; [REGION=&lt;aws-region&gt;] [USERNAME=&lt;username-tag&gt;] sentry-trigger-ebs-scan\n\nExample:\n  INSTANCE=i-0076dab31026905f5 sentry-trigger-ebs-scan\n</code></pre> <p>If you specify a <code>USERNAME</code> the snapshot(s) will be tagged accordingly. This should ease identifying your own snapshots if using a shared account. Default username is <code>cnctraining</code>.</p> <pre><code># Trigger scan of EC2 instance i-0076dab31026905f5 existing in the current region\nINSTANCE=i-0076dab31026905f5 sentry-trigger-ebs-scan\n</code></pre> <p>Example result:</p> <pre><code>Using region eu-central-1 for user cnctraining\nState machine is arn:aws:states:eu-central-1:634503960501:stateMachine:ScannerStateMachine-pueSSKvfdN4K\nInstance volume(s)\\nvol-03b25f8105caf9f00\nSnapshot snap-0f4cc9d1d8a094861 for volume vol-03b25f8105caf9f00 created\n{\n\"ScanID\": \"634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\",\n  \"ResourceType\": \"aws-ebs-volume\",\n  \"ResourceLocation\": \"snap-0f4cc9d1d8a094861\",\n  \"ResourceRegion\": \"eu-central-1\",\n  \"MetaData\": {\n\"AWSAccountID\": \"634503960501\",\n    \"SnapshotID\": \"snap-0f4cc9d1d8a094861\",\n    \"VolumeID\": \"vol-03b25f8105caf9f00\",\n    \"AttachedInstances\": [\n{\n\"InstanceID\": \"i-0076dab31026905f5\"\n}\n]\n}\n}\n{\n\"executionArn\": \"arn:aws:states:eu-central-1:634503960501:execution:ScannerStateMachine-pueSSKvfdN4K:Manual-EBS-resource-634503960501-635658bd-0deb-42a7-8594-1089d87bfc40\",\n    \"startDate\": \"2023-03-02T14:36:00.763000+00:00\"\n}\n</code></pre>"},{"location":"play/tools/#script-sentry-trigger-ecr-scan","title":"Script <code>sentry-trigger-ecr-scan</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used.</p> <p>Example calls:</p> <pre><code># Help\nsentry-trigger-ecr-scan help\n</code></pre> <pre><code>Please specify at least the ecr repository to be scanned.\n\nUsage: REPO=&lt;repo-name&gt; [TAG=&lt;image-tag] [REGION=&lt;aws-region&gt;] sentry-trigger-ecr-scan\n\nExample:\n  REPO=mawinkler/evil TAG=latest sentry-trigger-ecr-scan\n</code></pre> <pre><code># Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region\nREPO=mawinkler/evil TAG=0.1 sentry-trigger-ecr-scan\n</code></pre>"},{"location":"play/tools/#script-sentry-trigger-lambda-scan","title":"Script <code>sentry-trigger-lambda-scan</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used.</p> <p>Example calls:</p> <pre><code># Help\nsentry-trigger-lambda-scan help\n</code></pre> <pre><code>Please specify at least the lambda to be scanned.\n\nUsage: LAMBDA=&lt;lambda-name&gt; [REGION=&lt;aws-region&gt;] sentry-trigger-lambda-scan\n\nExample:\n  LAMBDA=cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan\n</code></pre> <pre><code># Trigger scan of ECR repo mawinkler/evil with tag 0.1 existing in the current region\nLAMBDA=cloud-sentry-EbsResourceConsumer sentry-trigger-lambda-scan\n</code></pre>"},{"location":"play/tools/#script-sentry-remove-snapshots","title":"Script <code>sentry-remove-snapshots</code>","text":"<p>The script is region-aware. This means unless you specify it the currently active AWS region from your shell will be used.</p> <p>Example calls:</p> <pre><code># Help\nsentry-remove-snapshots help\n</code></pre> <pre><code>Usage: [REGION=&lt;aws-region&gt;] [USERNAME=&lt;username-tag&gt;] sentry-remove-snapshots\n\nExample:\n  USERNAME=cnctraining sentry-remove-snapshots\n</code></pre> <p>Example result:</p> <pre><code>Using region eu-central-1 for user cnctraining\nSnapshot(s) to delete\nsnap-0f4cc9d1d8a094861\nsnap-00f0472b795c00644\nsnap-025d77b5303a37b9d\nsnap-0b357bbc2fef3edad\nsnap-07ee34bf66221579d\nsnap-0b4d1faead597e047\nDeleting snapshot snap-0f4cc9d1d8a094861\nDeleting snapshot snap-00f0472b795c00644\nDeleting snapshot snap-025d77b5303a37b9d\nDeleting snapshot snap-0b357bbc2fef3edad\nDeleting snapshot snap-07ee34bf66221579d\nDeleting snapshot snap-0b4d1faead597e047\n</code></pre>"},{"location":"play/tools/#repo-c1-sentry-reports-to-cloudwatch","title":"Repo C1 Sentry Reports to CloudWatch","text":"<p>Here, I'm describing a simple way to easily get new Sentry reports to CloudWatch using Lambda.</p> <p>Repo Link</p>"},{"location":"play/with-falco/","title":"Play with Falco &amp; Container Security","text":"<p>These examples are based on the default Falco ruleset and the additional rules provided by the playground.</p>"},{"location":"play/with-falco/#networking","title":"Networking","text":""},{"location":"play/with-falco/#pg-net-kubernetes-outbound-connection","title":"(PG-NET) Kubernetes Outbound Connection","text":"<p>Triggers, if a container is initiating an outbound network communication via TCP or UDP.</p> <pre><code>$ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash\nroot@nginx-6799fc88d8-n5tdd:/# curl www.google.com\n</code></pre>"},{"location":"play/with-falco/#kshell","title":"KShell","text":""},{"location":"play/with-falco/#pg-kshell-process-started-in-kshell-container","title":"(PG-KSHELL) Process started in kshell container","text":"<p>Triggers, if any process is run in the kshell pod</p> <pre><code>$ kubectl run -it --image=ubuntu kshell --restart=Never --labels=kshell=true --rm -- /bin/bash\nroot@kshell:/# tail /var/log/bootstrap.log </code></pre>"},{"location":"play/with-falco/#pg-kshell-file-or-directory-created-in-kshell-container","title":"(PG-KSHELL) File or directory created in kshell container","text":"<p>Triggers, if a file or directory is created in the kshell pod</p> <pre><code>$ kubectl run -it --image=ubuntu kshell --restart=Never --labels=kshell=true --rm -- /bin/bash\nroot@kshell:/# touch foo.txt\nroot@kshell:/# mkdir bar\n</code></pre>"},{"location":"play/with-falco/#dangerous-things","title":"Dangerous Things","text":""},{"location":"play/with-falco/#pg-ig-information-gathering-detected","title":"(PG-IG) Information gathering detected","text":"<p>Triggers, if one of the named tools (whoami, nmap, racoon) is run inside a container.</p> <pre><code>$ kubectl run -it busybox --image busybox -- /bin/sh\n/ # whoami\n</code></pre>"},{"location":"play/with-falco/#pg-shell-attachexec-pod-with-terminal-user-shell-in-container","title":"(PG-SHELL) Attach/Exec Pod with Terminal User shell in container","text":"<p>This rule triggers, if one attaches / executes a shell in a container not running as root.</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n  containers:\n  - name: sec-ctx-demo\n    image: busybox\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\n    securityContext:\n      allowPrivilegeEscalation: false\nEOF\n</code></pre> <p>and</p> <pre><code>$ kubectl exec -it security-context-demo -- /bin/sh\n</code></pre>"},{"location":"play/with-falco/#pg-shell-attachexec-pod-with-terminal-root-shell-in-container","title":"(PG-SHELL) Attach/Exec Pod with Terminal Root shell in container","text":"<p>This rule triggers, if one attaches / executes a shell in a container not running as root.</p> <pre><code>$ kubectl create namespace nginx\n$ kubectl -n nginx create deployment --image=nginx nginx\n$ kubectl -n nginx get pods\n</code></pre> <p>and</p> <pre><code>$ kubectl exec -it -n nginx nginx-6799fc88d8-n5tdd -- /bin/bash\n</code></pre>"},{"location":"play/with-falco/#pg-root-container-run-as-root-user","title":"(PG-ROOT) Container Run as Root User","text":"<p>Rule triggers, if container is started running as root</p> <pre><code>$ kubectl run -it busybox --image busybox -- /bin/sh\n</code></pre>"},{"location":"play/with-falco/#integrity-monitoring-in-containers","title":"Integrity Monitoring in Containers","text":""},{"location":"play/with-falco/#pg-imc-detect-new-file","title":"(PG-IMC) Detect New File","text":""},{"location":"play/with-falco/#pg-imc-detect-new-directory","title":"(PG-IMC) Detect New Directory","text":""},{"location":"play/with-falco/#pg-imc-detect-file-permission-or-ownership-change","title":"(PG-IMC) Detect File Permission or Ownership Change","text":""},{"location":"play/with-falco/#pg-imc-detect-directory-change","title":"(PG-IMC) Detect Directory Change","text":""},{"location":"play/with-falco/#integrity-monitoring-on-host-and-containers","title":"Integrity Monitoring on Host and Containers","text":""},{"location":"play/with-falco/#pg-im-kernel-module-modification","title":"(PG-IM) Kernel Module Modification","text":""},{"location":"play/with-falco/#pg-im-node-created-in-filesystem","title":"(PG-IM) Node Created in Filesystem","text":""},{"location":"play/with-falco/#pg-im-listen-on-new-port","title":"(PG-IM) Listen on New Port","text":""},{"location":"play/with-falco/#admin-activities","title":"Admin Activities","text":""},{"location":"play/with-falco/#pg-adm-detect-su-or-sudo","title":"(PG-ADM) Detect su or sudo","text":"<pre><code>$ sudo su -\n</code></pre>"},{"location":"play/with-falco/#pg-adm-package-management-launched","title":"(PG-ADM) Package Management Launched","text":"<pre><code>$ sudo apt update\n</code></pre>"},{"location":"play/with-falco/#ssh","title":"SSH","text":""},{"location":"play/with-falco/#pg-ssh-inbound-ssh-connection","title":"(PG-SSH) Inbound SSH Connection","text":""},{"location":"play/with-falco/#pg-ssh-outbound-ssh-connection","title":"(PG-SSH) Outbound SSH Connection","text":""},{"location":"play/with-falco/#miscellaneous","title":"Miscellaneous","text":""},{"location":"play/with-falco/#pg-kubectl-k8s-vulnerable-kubectl-copy","title":"(PG-KUBECTL) K8s Vulnerable Kubectl Copy","text":""}]}